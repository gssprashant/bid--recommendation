{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a041dc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All required libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Standard libraries\n",
    "    import os\n",
    "    import sys\n",
    "    import json\n",
    "    import pickle\n",
    "    import warnings\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    # Data manipulation\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # Machine learning\n",
    "    from xgboost import XGBRegressor\n",
    "    from sklearn.model_selection import TimeSeriesSplit\n",
    "    from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "    # Suppress warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    pd.set_option('display.max_columns', None)\n",
    "\n",
    "    print(\"✅ All required libraries imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importing libraries: {str(e)}\")\n",
    "    print(\"\\nPlease install missing packages using:\")\n",
    "    print(\"pip install pandas numpy scikit-learn xgboost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd0495",
   "metadata": {},
   "source": [
    "# Bid Recommendation System\n",
    "\n",
    "This notebook implements a machine learning model for bid recommendations with the following features:\n",
    "\n",
    "1. Data Loading & Validation\n",
    "   - Automatic CSV file detection\n",
    "   - Data structure validation\n",
    "   - Date parsing and sorting\n",
    "   - Basic statistics calculation\n",
    "\n",
    "2. Feature Engineering\n",
    "   - Categorical feature encoding\n",
    "   - Time-based features\n",
    "   - Rolling statistics\n",
    "\n",
    "3. Model Training\n",
    "   - XGBoost Regressor\n",
    "   - Time-series cross-validation\n",
    "   - Feature importance analysis\n",
    "\n",
    "4. Model Evaluation\n",
    "   - RMSE, MAE, R² metrics\n",
    "   - Error distribution analysis\n",
    "   - Prediction calibration\n",
    "\n",
    "5. Deployment Preparation\n",
    "   - Model artifact saving\n",
    "   - Encoder persistence\n",
    "   - Metadata tracking\n",
    "\n",
    "## Requirements\n",
    "\n",
    "The dataset should be a CSV file with the following columns:\n",
    "- `BidDate`: Date of the bid\n",
    "- `BidAmount`: Numerical bid amount\n",
    "- `WinStatus`: Binary indicator (0/1) for bid success\n",
    "\n",
    "Place your CSV file in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f816c4ec",
   "metadata": {},
   "source": [
    "# Bid Recommendation Model\n",
    "\n",
    "This notebook implements a machine learning model for bid recommendations. \n",
    "\n",
    "## Setup Requirements\n",
    "1. Python 3.7+\n",
    "2. Required packages: pandas, numpy, scikit-learn, xgboost\n",
    "3. Data file: 'bid_data.csv' in the same directory\n",
    "\n",
    "## Instructions\n",
    "1. Run cells in order\n",
    "2. Make sure bid_data.csv contains these columns:\n",
    "   - BidDate (datetime)\n",
    "   - BidAmount (numeric)\n",
    "   - WinStatus (1/0)\n",
    "   - ProjectType (categorical)\n",
    "   - Location (categorical)\n",
    "   - ClientType (categorical)\n",
    "\n",
    "## Important Notes\n",
    "- The model uses time-based cross-validation\n",
    "- All amounts are in USD\n",
    "- Missing values will be handled automatically\n",
    "- Models and artifacts will be saved in ./models directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3c131842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine learning\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4bc3f9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sample dataset...\n",
      "\n",
      "Sample data saved to sample_bid_data.csv\n",
      "\n",
      "Dataset Statistics:\n",
      "Number of records: 1,000\n",
      "\n",
      "Sample data saved to sample_bid_data.csv\n",
      "\n",
      "Dataset Statistics:\n",
      "Number of records: 1,000\n",
      "Date range: 2024-01-01 00:00:00 to 2026-09-26 00:00:00\n",
      "Average bid amount: $99,880.61\n",
      "Win rate: 36.9%\n",
      "\n",
      "Sample data created successfully!\n",
      "Date range: 2024-01-01 00:00:00 to 2026-09-26 00:00:00\n",
      "Average bid amount: $99,880.61\n",
      "Win rate: 36.9%\n",
      "\n",
      "Sample data created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create sample data for testing\n",
    "def create_sample_data(n_samples=1000):\n",
    "    \"\"\"Create a sample dataset for testing the bid recommendation system\"\"\"\n",
    "    try:\n",
    "        print(\"Creating sample dataset...\")\n",
    "        \n",
    "        # Set random seed for reproducibility\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Generate dates\n",
    "        start_date = pd.Timestamp('2024-01-01')\n",
    "        dates = pd.date_range(start=start_date, periods=n_samples, freq='D')\n",
    "        \n",
    "        # Create project types\n",
    "        project_types = ['Commercial', 'Residential', 'Industrial', 'Infrastructure']\n",
    "        locations = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']\n",
    "        client_types = ['Government', 'Private', 'Corporate', 'Non-Profit']\n",
    "        \n",
    "        # Generate sample data\n",
    "        df = pd.DataFrame({\n",
    "            'BidDate': dates,\n",
    "            'ProjectType': np.random.choice(project_types, size=n_samples),\n",
    "            'Location': np.random.choice(locations, size=n_samples),\n",
    "            'ClientType': np.random.choice(client_types, size=n_samples),\n",
    "            'BidAmount': np.random.lognormal(mean=11, sigma=1, size=n_samples),  # Log-normal for realistic bid amounts\n",
    "            'EstimatedCost': np.random.lognormal(mean=10.8, sigma=1, size=n_samples),\n",
    "            'CompetitorCount': np.random.randint(1, 8, size=n_samples)\n",
    "        })\n",
    "        \n",
    "        # Generate win status based on a simple model\n",
    "        prob_win = 1 / (1 + np.exp(-(\n",
    "            -0.3  # Base rate\n",
    "            + 0.2 * (df['BidAmount'] < df['EstimatedCost']).astype(int)  # Lower bids more likely to win\n",
    "            - 0.1 * df['CompetitorCount']  # More competitors reduce win chance\n",
    "            + np.random.normal(0, 0.5, n_samples)  # Random noise\n",
    "        )))\n",
    "        df['WinStatus'] = (np.random.random(n_samples) < prob_win).astype(int)\n",
    "        \n",
    "        # Save to CSV\n",
    "        csv_path = 'sample_bid_data.csv'\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"\\nSample data saved to {csv_path}\")\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\nDataset Statistics:\")\n",
    "        print(f\"Number of records: {len(df):,}\")\n",
    "        print(f\"Date range: {df['BidDate'].min()} to {df['BidDate'].max()}\")\n",
    "        print(f\"Average bid amount: ${df['BidAmount'].mean():,.2f}\")\n",
    "        print(f\"Win rate: {(df['WinStatus'].mean() * 100):.1f}%\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating sample data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Create sample dataset\n",
    "try:\n",
    "    df = create_sample_data(n_samples=1000)\n",
    "    print(\"\\nSample data created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create sample data: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4593afa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up project configuration...\n",
      "Created directory: c:\\Users\\prash\\bid--recommendation\\GSS Bid Models\\models\n",
      "Created directory: c:\\Users\\prash\\bid--recommendation\\GSS Bid Models\\data\n",
      "Created directory: c:\\Users\\prash\\bid--recommendation\\GSS Bid Models\\config\n",
      "Created directory: c:\\Users\\prash\\bid--recommendation\\GSS Bid Models\\deploy\n",
      "\n",
      "Configuration saved to: c:\\Users\\prash\\bid--recommendation\\GSS Bid Models\\config\\config.json\n",
      "\n",
      "Project setup completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration and Path Setup\n",
    "def setup_project():\n",
    "    \"\"\"Setup project configuration and ensure all prerequisites are met\"\"\"\n",
    "    try:\n",
    "        print(\"Setting up project configuration...\")\n",
    "        \n",
    "        # 1. Create configuration dictionary\n",
    "        config = {\n",
    "            'data': {\n",
    "                'required_columns': ['BidDate', 'BidAmount', 'WinStatus'],\n",
    "                'optional_columns': ['ProjectType', 'Location', 'ClientType', 'EstimatedCost', 'CompetitorCount']\n",
    "            },\n",
    "            'model': {\n",
    "                'random_state': 42,\n",
    "                'cv_folds': 5,\n",
    "                'test_size': 0.2\n",
    "            },\n",
    "            'paths': {}\n",
    "        }\n",
    "        \n",
    "        # 2. Setup directory structure\n",
    "        base_dir = Path().absolute()\n",
    "        directories = {\n",
    "            'models': base_dir / 'models',\n",
    "            'data': base_dir / 'data',\n",
    "            'config': base_dir / 'config',\n",
    "            'deploy': base_dir / 'deploy'\n",
    "        }\n",
    "        \n",
    "        # Create directories\n",
    "        for name, path in directories.items():\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "            config['paths'][name] = path\n",
    "            print(f\"Created directory: {path}\")\n",
    "        \n",
    "        # 3. Define file paths\n",
    "        config['paths'].update({\n",
    "            'model_file': directories['models'] / 'xgb_regressor.pkl',\n",
    "            'encoders_file': directories['models'] / 'encoders.pkl',\n",
    "            'metadata_file': directories['config'] / 'metadata.json',\n",
    "            'predictions_file': directories['deploy'] / 'predictions.csv'\n",
    "        })\n",
    "        \n",
    "        # 4. Check for sample data\n",
    "        if not (base_dir / 'sample_bid_data.csv').exists():\n",
    "            print(\"\\nNo sample data found. You can create it using:\")\n",
    "            print(\"df = create_sample_data(n_samples=1000)\")\n",
    "        \n",
    "        # 5. Save configuration\n",
    "        config_file = directories['config'] / 'config.json'\n",
    "        with open(config_file, 'w') as f:\n",
    "            json.dump(config, f, indent=4, default=str)\n",
    "        \n",
    "        print(\"\\nConfiguration saved to:\", config_file)\n",
    "        return config\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up project: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Setup project configuration\n",
    "try:\n",
    "    config = setup_project()\n",
    "    print(\"\\nProject setup completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to setup project: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1e9fc9",
   "metadata": {},
   "source": [
    "# Bid Fee Prediction Model\n",
    "\n",
    "This notebook implements a machine learning model to predict bid fees based on historical data.\n",
    "\n",
    "## Features Used\n",
    "- Property characteristics (Type, Location, Size)\n",
    "- Market indicators (Population, Income, Business stats)\n",
    "- Historical bid patterns\n",
    "- ZIP code patterns\n",
    "\n",
    "## Model Components\n",
    "1. Bid Fee Regressor: Predicts optimal bid amount\n",
    "2. Win Probability Classifier: Estimates chance of winning (if data available)\n",
    "\n",
    "## Key Requirements\n",
    "- Input data must include: ZipCode, BidFee (for training)\n",
    "- Optional but valuable: PropertyType, Market metrics, WinProbability\n",
    "\n",
    "## Usage\n",
    "1. Run cells in order\n",
    "2. Models will be saved to `models/` directory\n",
    "3. Use `BidPrediction` class for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "313fc1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available data files:\n",
      "1. sample_bid_data.csv\n",
      "\n",
      "Loading data from: sample_bid_data.csv\n",
      "\n",
      "Initial dataframe info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   BidDate          1000 non-null   object \n",
      " 1   ProjectType      1000 non-null   object \n",
      " 2   Location         1000 non-null   object \n",
      " 3   ClientType       1000 non-null   object \n",
      " 4   BidAmount        1000 non-null   float64\n",
      " 5   EstimatedCost    1000 non-null   float64\n",
      " 6   CompetitorCount  1000 non-null   int64  \n",
      " 7   WinStatus        1000 non-null   int64  \n",
      "dtypes: float64(2), int64(2), object(4)\n",
      "memory usage: 62.6+ KB\n",
      "None\n",
      "\n",
      "Dataset Statistics:\n",
      "Number of records: 1,000\n",
      "Date range: 2024-01-01 00:00:00 to 2026-09-26 00:00:00\n",
      "Average bid amount: $99,880.61\n",
      "Win rate: 36.9%\n",
      "\n",
      "Data loaded and validated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Data Loading and Validation\n",
    "def load_and_validate_data():\n",
    "    \"\"\"Load and validate bid data\"\"\"\n",
    "    try:\n",
    "        # First check current directory for bid data files\n",
    "        data_files = [f for f in os.listdir() if f.endswith('.csv')]\n",
    "        \n",
    "        if not data_files:\n",
    "            raise FileNotFoundError(\"No CSV files found in the current directory\")\n",
    "            \n",
    "        print(\"Available data files:\")\n",
    "        for i, file in enumerate(data_files, 1):\n",
    "            print(f\"{i}. {file}\")\n",
    "            \n",
    "        # If multiple files found, use the first one\n",
    "        csv_path = data_files[0]\n",
    "        print(f\"\\nLoading data from: {csv_path}\")\n",
    "        \n",
    "        # Load data with error handling\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            raise ValueError(f\"The file {csv_path} is empty\")\n",
    "        except pd.errors.ParserError:\n",
    "            raise ValueError(f\"Error parsing {csv_path}. Make sure it's a valid CSV file\")\n",
    "            \n",
    "        # Display initial info\n",
    "        print(\"\\nInitial dataframe info:\")\n",
    "        print(df.info())\n",
    "        \n",
    "        # Check columns\n",
    "        expected_cols = ['BidDate', 'BidAmount', 'WinStatus']\n",
    "        missing_cols = [col for col in expected_cols if col not in df.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(\"\\nWarning: Missing expected columns:\", missing_cols)\n",
    "            print(\"Available columns:\", df.columns.tolist())\n",
    "            \n",
    "            # Try to identify similar column names\n",
    "            all_cols = df.columns.str.lower()\n",
    "            for missing in missing_cols:\n",
    "                similar = [col for col in df.columns if missing.lower() in col.lower()]\n",
    "                if similar:\n",
    "                    print(f\"Possible matches for {missing}:\", similar)\n",
    "            \n",
    "            raise ValueError(f\"Required columns missing: {missing_cols}\")\n",
    "        \n",
    "        # Convert date column\n",
    "        try:\n",
    "            df['BidDate'] = pd.to_datetime(df['BidDate'])\n",
    "        except Exception as e:\n",
    "            print(\"Error converting BidDate column:\", str(e))\n",
    "            print(\"Sample values from BidDate column:\", df['BidDate'].head())\n",
    "            raise ValueError(\"Could not parse BidDate column\")\n",
    "        \n",
    "        # Sort by date\n",
    "        df = df.sort_values('BidDate').reset_index(drop=True)\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing = df[expected_cols].isnull().sum()\n",
    "        if missing.any():\n",
    "            print(\"\\nWarning: Missing values detected:\")\n",
    "            print(missing[missing > 0])\n",
    "            \n",
    "        # Basic data validation\n",
    "        if len(df) == 0:\n",
    "            raise ValueError(\"DataFrame is empty after loading\")\n",
    "            \n",
    "        # Print basic statistics\n",
    "        print(\"\\nDataset Statistics:\")\n",
    "        print(f\"Number of records: {len(df):,}\")\n",
    "        print(f\"Date range: {df['BidDate'].min()} to {df['BidDate'].max()}\")\n",
    "        print(f\"Average bid amount: ${df['BidAmount'].mean():,.2f}\")\n",
    "        print(f\"Win rate: {(df['WinStatus'].mean() * 100):.1f}%\")\n",
    "        \n",
    "        # Save the path for future reference\n",
    "\n",
    "        csv_path = os.path.abspath(csv_path)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in data loading: {str(e)}\")\n",
    "        print(\"\\nPlease ensure:\")\n",
    "        print(\"1. Your CSV file is in the current directory\")\n",
    "        print(\"2. The file contains the required columns: BidDate, BidAmount, WinStatus\")\n",
    "        print(\"3. The BidDate column contains valid dates\")\n",
    "        print(\"4. The BidAmount column contains numeric values\")\n",
    "        print(\"5. The WinStatus column contains binary values (0/1)\")\n",
    "        raise\n",
    "\n",
    "# Load and validate the data\n",
    "try:\n",
    "    df = load_and_validate_data()\n",
    "    print(\"\\nData loaded and validated successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nFailed to load data: {str(e)}\")\n",
    "    print(\"\\nPlease fix the data issues and try again.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b37880fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ProjectType:\n",
      "ProjectType\n",
      "Infrastructure    280\n",
      "Commercial        258\n",
      "Industrial        232\n",
      "Residential       230\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ProjectType Mapping (first 5):\n",
      "Industrial -> 1\n",
      "Infrastructure -> 2\n",
      "Commercial -> 0\n",
      "Residential -> 3\n",
      "\n",
      "Processing Location:\n",
      "Location\n",
      "New York       213\n",
      "Phoenix        209\n",
      "Chicago        205\n",
      "Los Angeles    202\n",
      "Houston        171\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Location Mapping (first 5):\n",
      "New York -> 3\n",
      "Phoenix -> 4\n",
      "Houston -> 1\n",
      "Chicago -> 0\n",
      "Los Angeles -> 2\n",
      "\n",
      "Processing ClientType:\n",
      "ClientType\n",
      "Private       262\n",
      "Government    258\n",
      "Non-Profit    251\n",
      "Corporate     229\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ClientType Mapping (first 5):\n",
      "Private -> 3\n",
      "Corporate -> 0\n",
      "Non-Profit -> 2\n",
      "Government -> 1\n",
      "\n",
      "Categorical features processed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering - Part 1: Categorical Features\n",
    "def process_categorical_features(df, categorical_cols=None):\n",
    "    \"\"\"Process categorical features with label encoding\"\"\"\n",
    "    try:\n",
    "        if categorical_cols is None:\n",
    "            # Identify categorical columns (customize this list based on your data)\n",
    "            categorical_cols = ['ProjectType', 'Location', 'ClientType']\n",
    "            \n",
    "        encoders = {}\n",
    "        df_encoded = df.copy()\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if col in df.columns:\n",
    "                print(f\"\\nProcessing {col}:\")\n",
    "                # Print value counts\n",
    "                print(df[col].value_counts().head())\n",
    "                \n",
    "                # Create and fit encoder\n",
    "                encoders[col] = LabelEncoder()\n",
    "                df_encoded[col] = encoders[col].fit_transform(df[col].fillna('MISSING'))\n",
    "                \n",
    "                # Print mapping for verification\n",
    "                unique_values = df[col].unique()\n",
    "                encoded_values = encoders[col].transform(unique_values)\n",
    "                mapping = dict(zip(unique_values, encoded_values))\n",
    "                print(f\"\\n{col} Mapping (first 5):\")\n",
    "                for k, v in list(mapping.items())[:5]:\n",
    "                    print(f\"{k} -> {v}\")\n",
    "                    \n",
    "        return df_encoded, encoders, categorical_cols\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing categorical features: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Process categorical features\n",
    "try:\n",
    "    df_encoded, encoders, categorical_cols = process_categorical_features(df)\n",
    "    print(\"\\nCategorical features processed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to process categorical features: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "efc5e09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating 7-day rolling features...\n",
      "\n",
      "Calculating 30-day rolling features...\n",
      "\n",
      "Calculating 90-day rolling features...\n",
      "\n",
      "Time features created:\n",
      "\n",
      "Basic time features: ['Year', 'Month', 'DayOfWeek', 'Quarter']\n",
      "\n",
      "Rolling features: ['RollingMeanBid_7d', 'RollingWinRate_7d', 'RollingBidCount_7d', 'RollingMeanBid_30d', 'RollingWinRate_30d', 'RollingBidCount_30d', 'RollingMeanBid_90d', 'RollingWinRate_90d', 'RollingBidCount_90d']\n",
      "\n",
      "Time-based features created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering - Part 2: Time-based Features\n",
    "def create_time_features(df):\n",
    "    \"\"\"Create time-based features and rolling statistics\"\"\"\n",
    "    try:\n",
    "        df_time = df.copy()\n",
    "        \n",
    "        # Basic time features\n",
    "        df_time['Year'] = df_time['BidDate'].dt.year\n",
    "        df_time['Month'] = df_time['BidDate'].dt.month\n",
    "        df_time['DayOfWeek'] = df_time['BidDate'].dt.dayofweek\n",
    "        df_time['Quarter'] = df_time['BidDate'].dt.quarter\n",
    "        \n",
    "        # Rolling statistics\n",
    "        windows = [7, 30, 90]  # Days for rolling windows\n",
    "        for window in windows:\n",
    "            print(f\"\\nCalculating {window}-day rolling features...\")\n",
    "            \n",
    "            # Rolling mean bid amount\n",
    "            df_time[f'RollingMeanBid_{window}d'] = (\n",
    "                df_time['BidAmount']\n",
    "                .rolling(window=window, min_periods=1)\n",
    "                .mean()\n",
    "            )\n",
    "            \n",
    "            # Rolling win rate\n",
    "            df_time[f'RollingWinRate_{window}d'] = (\n",
    "                df_time['WinStatus']\n",
    "                .rolling(window=window, min_periods=1)\n",
    "                .mean()\n",
    "            )\n",
    "            \n",
    "            # Rolling bid count\n",
    "            df_time[f'RollingBidCount_{window}d'] = (\n",
    "                df_time['BidAmount']\n",
    "                .rolling(window=window, min_periods=1)\n",
    "                .count()\n",
    "            )\n",
    "        \n",
    "        # Fill any NaN values from rolling calculations\n",
    "        rolling_cols = [col for col in df_time.columns if 'Rolling' in col]\n",
    "        df_time[rolling_cols] = df_time[rolling_cols].fillna(method='bfill').fillna(0)\n",
    "        \n",
    "        print(\"\\nTime features created:\")\n",
    "        print(\"\\nBasic time features:\", ['Year', 'Month', 'DayOfWeek', 'Quarter'])\n",
    "        print(\"\\nRolling features:\", rolling_cols)\n",
    "        \n",
    "        return df_time\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating time features: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Create time-based features\n",
    "try:\n",
    "    df_features = create_time_features(df_encoded)\n",
    "    print(\"\\nTime-based features created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create time features: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d267de33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected features: ['ProjectType', 'Location', 'ClientType', 'Year', 'Month', 'DayOfWeek', 'Quarter', 'RollingMeanBid_7d', 'RollingWinRate_7d', 'RollingBidCount_7d', 'RollingMeanBid_30d', 'RollingWinRate_30d', 'RollingBidCount_30d', 'RollingMeanBid_90d', 'RollingWinRate_90d', 'RollingBidCount_90d']\n",
      "\n",
      "Feature Statistics:\n",
      "       ProjectType  Location  ClientType     Year    Month  DayOfWeek  \\\n",
      "count      1000.00   1000.00     1000.00  1000.00  1000.00     1000.0   \n",
      "mean          1.48      2.05        1.55  2024.90     6.10        3.0   \n",
      "std           1.11      1.43        1.11     0.79     3.31        2.0   \n",
      "min           0.00      0.00        0.00  2024.00     1.00        0.0   \n",
      "25%           0.00      1.00        1.00  2024.00     3.00        1.0   \n",
      "50%           2.00      2.00        2.00  2025.00     6.00        3.0   \n",
      "75%           2.00      3.00        3.00  2026.00     9.00        5.0   \n",
      "max           3.00      4.00        3.00  2026.00    12.00        6.0   \n",
      "\n",
      "       Quarter  RollingMeanBid_7d  RollingWinRate_7d  RollingBidCount_7d  \\\n",
      "count  1000.00            1000.00            1000.00             1000.00   \n",
      "mean      2.37           99903.85               0.37                6.98   \n",
      "std       1.07           46793.05               0.16                0.30   \n",
      "min       1.00           28295.78               0.00                1.00   \n",
      "25%       1.00           71211.34               0.29                7.00   \n",
      "50%       2.00           89529.21               0.43                7.00   \n",
      "75%       3.00          114647.74               0.43                7.00   \n",
      "max       4.00          325017.83               1.00                7.00   \n",
      "\n",
      "       RollingMeanBid_30d  RollingWinRate_30d  RollingBidCount_30d  \\\n",
      "count             1000.00             1000.00              1000.00   \n",
      "mean            100058.74                0.37                29.56   \n",
      "std              22712.91                0.09                 2.89   \n",
      "min              37949.21                0.17                 1.00   \n",
      "25%              82531.21                0.30                30.00   \n",
      "50%              96598.30                0.37                30.00   \n",
      "75%             113633.45                0.43                30.00   \n",
      "max             164485.04                1.00                30.00   \n",
      "\n",
      "       RollingMeanBid_90d  RollingWinRate_90d  RollingBidCount_90d  \n",
      "count             1000.00             1000.00              1000.00  \n",
      "mean            100858.57                0.38                86.00  \n",
      "std              12880.70                0.07                14.94  \n",
      "min              37949.21                0.28                 1.00  \n",
      "25%              89860.70                0.33                90.00  \n",
      "50%             101256.93                0.37                90.00  \n",
      "75%             110903.19                0.40                90.00  \n",
      "max             129774.91                1.00                90.00  \n",
      "\n",
      "Top 10 feature correlations with bid amount:\n",
      "                feature  correlation\n",
      "7     RollingMeanBid_7d     0.369217\n",
      "10   RollingMeanBid_30d     0.191505\n",
      "13   RollingMeanBid_90d     0.104371\n",
      "8     RollingWinRate_7d     0.061241\n",
      "11   RollingWinRate_30d     0.043376\n",
      "3                  Year    -0.042586\n",
      "15  RollingBidCount_90d    -0.025278\n",
      "9    RollingBidCount_7d     0.024222\n",
      "0           ProjectType    -0.017219\n",
      "1              Location     0.011977\n",
      "\n",
      "Feature matrices prepared successfully!\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection and Matrix Preparation\n",
    "def prepare_feature_matrices(df, categorical_cols):\n",
    "    \"\"\"Prepare feature matrices for training\"\"\"\n",
    "    try:\n",
    "        # Define feature columns\n",
    "        time_cols = ['Year', 'Month', 'DayOfWeek', 'Quarter']\n",
    "        rolling_cols = [col for col in df.columns if 'Rolling' in col]\n",
    "        \n",
    "        # Combine all feature columns\n",
    "        feature_cols = categorical_cols + time_cols + rolling_cols\n",
    "        print(\"\\nSelected features:\", feature_cols)\n",
    "        \n",
    "        # Create feature matrix X\n",
    "        X = df[feature_cols].copy()\n",
    "        \n",
    "        # Create target variables\n",
    "        y_reg = df['BidAmount']  # For regression (bid amount)\n",
    "        \n",
    "        # Basic feature statistics\n",
    "        print(\"\\nFeature Statistics:\")\n",
    "        print(X.describe().round(2))\n",
    "        \n",
    "        # Check for any remaining missing values\n",
    "        missing = X.isnull().sum()\n",
    "        if missing.any():\n",
    "            print(\"\\nWarning: Missing values detected in features:\")\n",
    "            print(missing[missing > 0])\n",
    "            \n",
    "        # Simple correlation analysis with target\n",
    "        correlations = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'correlation': [X[col].corr(y_reg) for col in feature_cols]\n",
    "        }).sort_values('correlation', key=abs, ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 10 feature correlations with bid amount:\")\n",
    "        print(correlations.head(10))\n",
    "        \n",
    "        return X, y_reg, feature_cols\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing feature matrices: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Prepare feature matrices\n",
    "try:\n",
    "    X, y_reg, feature_cols = prepare_feature_matrices(df_features, categorical_cols)\n",
    "    print(\"\\nFeature matrices prepared successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to prepare feature matrices: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5d309273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cross-validation...\n",
      "\n",
      "Fold 1:\n",
      "Train size: 170, Test size: 166\n",
      "RMSE: $152,896.02\n",
      "MAE: $93,466.27\n",
      "R2: -0.063\n",
      "\n",
      "Fold 2:\n",
      "Train size: 336, Test size: 166\n",
      "RMSE: $152,896.02\n",
      "MAE: $93,466.27\n",
      "R2: -0.063\n",
      "\n",
      "Fold 2:\n",
      "Train size: 336, Test size: 166\n",
      "RMSE: $95,308.43\n",
      "MAE: $64,744.55\n",
      "R2: -0.215\n",
      "\n",
      "Fold 3:\n",
      "Train size: 502, Test size: 166\n",
      "RMSE: $95,308.43\n",
      "MAE: $64,744.55\n",
      "R2: -0.215\n",
      "\n",
      "Fold 3:\n",
      "Train size: 502, Test size: 166\n",
      "RMSE: $146,261.13\n",
      "MAE: $84,980.97\n",
      "R2: 0.133\n",
      "\n",
      "Fold 4:\n",
      "Train size: 668, Test size: 166\n",
      "RMSE: $146,261.13\n",
      "MAE: $84,980.97\n",
      "R2: 0.133\n",
      "\n",
      "Fold 4:\n",
      "Train size: 668, Test size: 166\n",
      "RMSE: $103,259.17\n",
      "MAE: $71,340.92\n",
      "R2: 0.009\n",
      "\n",
      "Fold 5:\n",
      "Train size: 834, Test size: 166\n",
      "RMSE: $103,259.17\n",
      "MAE: $71,340.92\n",
      "R2: 0.009\n",
      "\n",
      "Fold 5:\n",
      "Train size: 834, Test size: 166\n",
      "RMSE: $127,446.93\n",
      "MAE: $82,243.77\n",
      "R2: -0.014\n",
      "\n",
      "Cross-validation Average Results:\n",
      "Average RMSE: $125,034.34 ± $22,761.63\n",
      "Average MAE: $79,355.30 ± $10,166.76\n",
      "Average R2: -0.030 ± 0.113\n",
      "\n",
      "Model training completed successfully!\n",
      "RMSE: $127,446.93\n",
      "MAE: $82,243.77\n",
      "R2: -0.014\n",
      "\n",
      "Cross-validation Average Results:\n",
      "Average RMSE: $125,034.34 ± $22,761.63\n",
      "Average MAE: $79,355.30 ± $10,166.76\n",
      "Average R2: -0.030 ± 0.113\n",
      "\n",
      "Model training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Model Training and Cross-validation\n",
    "def train_model(X, y, feature_cols):\n",
    "    \"\"\"Train XGBoost regressor with cross-validation\"\"\"\n",
    "    try:\n",
    "        # Initialize time-series cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        cv_scores = {\n",
    "            'rmse': [],\n",
    "            'mae': [],\n",
    "            'r2': []\n",
    "        }\n",
    "        \n",
    "        # Initialize model with robust parameters\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            min_child_weight=2,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        print(\"Starting cross-validation...\")\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            \n",
    "            # Ensure non-negative target values\n",
    "            y_train = y_train.clip(lower=0)\n",
    "            y_test = y_test.clip(lower=0)\n",
    "            \n",
    "            print(f\"\\nFold {fold}:\")\n",
    "            print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
    "            \n",
    "            # Train model\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred = np.maximum(y_pred, 0)  # Ensure non-negative predictions\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            cv_scores['rmse'].append(rmse)\n",
    "            cv_scores['mae'].append(mae)\n",
    "            cv_scores['r2'].append(r2)\n",
    "            \n",
    "            print(f\"RMSE: ${rmse:,.2f}\")\n",
    "            print(f\"MAE: ${mae:,.2f}\")\n",
    "            print(f\"R2: {r2:.3f}\")\n",
    "        \n",
    "        # Print average scores\n",
    "        print(\"\\nCross-validation Average Results:\")\n",
    "        print(f\"Average RMSE: ${np.mean(cv_scores['rmse']):,.2f} ± ${np.std(cv_scores['rmse']):,.2f}\")\n",
    "        print(f\"Average MAE: ${np.mean(cv_scores['mae']):,.2f} ± ${np.std(cv_scores['mae']):,.2f}\")\n",
    "        print(f\"Average R2: {np.mean(cv_scores['r2']):.3f} ± {np.std(cv_scores['r2']):.3f}\")\n",
    "        \n",
    "        return model, cv_scores\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in model training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    model, cv_scores = train_model(X, y_reg, feature_cols)\n",
    "    print(\"\\nModel training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to train model: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "38b26606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing model performance...\n",
      "\n",
      "Overall Performance Metrics:\n",
      "RMSE: $60,072.46\n",
      "MAE: $33,897.30\n",
      "R2 Score: 0.778\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "               feature  importance\n",
      "0          ProjectType    0.122013\n",
      "7    RollingMeanBid_7d    0.109138\n",
      "5            DayOfWeek    0.103015\n",
      "13  RollingMeanBid_90d    0.070718\n",
      "11  RollingWinRate_30d    0.070135\n",
      "2           ClientType    0.065170\n",
      "4                Month    0.063771\n",
      "10  RollingMeanBid_30d    0.062399\n",
      "1             Location    0.061835\n",
      "8    RollingWinRate_7d    0.060781\n",
      "\n",
      "Error Distribution:\n",
      "25th percentile: $9,506.50\n",
      "Median error: $21,090.10\n",
      "75th percentile: $40,710.43\n",
      "90th percentile: $75,801.98\n",
      "95th percentile: $99,934.75\n",
      "99th percentile: $192,752.20\n",
      "\n",
      "Prediction Calibration Analysis:\n",
      "                               predicted               actual\n",
      "                                    mean count           mean\n",
      "(3214.966, 41457.214]       33527.226562   100   20763.656998\n",
      "(41457.214, 50317.274]      46242.910156   100   33251.537094\n",
      "(50317.274, 58567.204]      54639.156250   100   42344.233983\n",
      "(58567.204, 66587.719]      62686.250000   100   48118.484207\n",
      "(66587.719, 76523.059]      71515.976562   100   62422.785286\n",
      "(76523.059, 87416.119]      81369.648438   100   74989.097229\n",
      "(87416.119, 102467.848]     94131.039062   100   98184.878215\n",
      "(102467.848, 126314.778]   113775.523438   100  107532.392927\n",
      "(126314.778, 185277.044]   150509.984375   100  171715.252833\n",
      "(185277.044, 1357029.875]  324224.093750   100  339483.755161\n",
      "\n",
      "Model analysis completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Model Analysis and Diagnostics\n",
    "def analyze_model(model, X, y, feature_cols):\n",
    "    \"\"\"Analyze model performance and generate diagnostics\"\"\"\n",
    "    try:\n",
    "        print(\"Analyzing model performance...\")\n",
    "        \n",
    "        # Generate predictions on full dataset\n",
    "        y_pred = model.predict(X)\n",
    "        y_pred = np.maximum(y_pred, 0)  # Ensure non-negative predictions\n",
    "        \n",
    "        # Calculate error metrics\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        \n",
    "        print(\"\\nOverall Performance Metrics:\")\n",
    "        print(f\"RMSE: ${rmse:,.2f}\")\n",
    "        print(f\"MAE: ${mae:,.2f}\")\n",
    "        print(f\"R2 Score: {r2:.3f}\")\n",
    "        \n",
    "        # Feature importance analysis\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 10 Most Important Features:\")\n",
    "        print(importance_df.head(10))\n",
    "        \n",
    "        # Error analysis\n",
    "        errors = y - y_pred\n",
    "        error_percentiles = np.percentile(np.abs(errors), [25, 50, 75, 90, 95, 99])\n",
    "        \n",
    "        print(\"\\nError Distribution:\")\n",
    "        print(f\"25th percentile: ${error_percentiles[0]:,.2f}\")\n",
    "        print(f\"Median error: ${error_percentiles[1]:,.2f}\")\n",
    "        print(f\"75th percentile: ${error_percentiles[2]:,.2f}\")\n",
    "        print(f\"90th percentile: ${error_percentiles[3]:,.2f}\")\n",
    "        print(f\"95th percentile: ${error_percentiles[4]:,.2f}\")\n",
    "        print(f\"99th percentile: ${error_percentiles[5]:,.2f}\")\n",
    "        \n",
    "        # Prediction calibration analysis\n",
    "        pred_ranges = pd.qcut(y_pred, q=10)\n",
    "        calibration_df = pd.DataFrame({\n",
    "            'predicted': y_pred,\n",
    "            'actual': y\n",
    "        }).groupby(pred_ranges).agg({\n",
    "            'predicted': ['mean', 'count'],\n",
    "            'actual': 'mean'\n",
    "        })\n",
    "        \n",
    "        print(\"\\nPrediction Calibration Analysis:\")\n",
    "        print(calibration_df)\n",
    "        \n",
    "        # Save analysis results\n",
    "        analysis_results = {\n",
    "            'metrics': {\n",
    "                'rmse': rmse,\n",
    "                'mae': mae,\n",
    "                'r2': r2\n",
    "            },\n",
    "            'feature_importance': importance_df,\n",
    "            'error_percentiles': error_percentiles,\n",
    "            'calibration': calibration_df\n",
    "        }\n",
    "        \n",
    "        return analysis_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in model analysis: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Analyze the model\n",
    "try:\n",
    "    analysis_results = analyze_model(model, X, y_reg, feature_cols)\n",
    "    print(\"\\nModel analysis completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to analyze model: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1334cc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model artifacts...\n",
      "Model saved to: c:\\Users\\prash\\bid--recommendation\\GSS Bid Models\\deploy\\models\\bid_fee_model.pkl\n",
      "Encoders saved to: c:\\Users\\prash\\bid--recommendation\\GSS Bid Models\\deploy\\models\\encoders.pkl\n",
      "Metadata saved to: c:\\Users\\prash\\bid--recommendation\\GSS Bid Models\\deploy\\config\\metadata.json\n",
      "\n",
      "Model and artifacts saved successfully!\n",
      "\n",
      "Model is ready for deployment with the following files:\n",
      "- Model: c:\\Users\\prash\\bid--recommendation\\GSS Bid Models\\deploy\\models\\bid_fee_model.pkl\n",
      "- Encoders: c:\\Users\\prash\\bid--recommendation\\GSS Bid Models\\deploy\\models\\encoders.pkl\n",
      "- Metadata: c:\\Users\\prash\\bid--recommendation\\GSS Bid Models\\deploy\\config\\metadata.json\n",
      "- Encoders: c:\\Users\\prash\\bid--recommendation\\GSS Bid Models\\deploy\\models\\encoders.pkl\n",
      "- Metadata: c:\\Users\\prash\\bid--recommendation\\GSS Bid Models\\deploy\\config\\metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Model Saving and Deployment Setup\n",
    "def save_model_artifacts(model, config, artifacts):\n",
    "    \"\"\"Save model and related artifacts\"\"\"\n",
    "    try:\n",
    "        print(\"Saving model artifacts...\")\n",
    "        \n",
    "        # Create deployment directory if it doesn't exist\n",
    "        config['deploy_dir'].mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        with open(config['model_path'], 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"Model saved to: {config['model_path']}\")\n",
    "        \n",
    "        # Save encoders\n",
    "        with open(config['encoders_path'], 'wb') as f:\n",
    "            pickle.dump(artifacts['encoders'], f)\n",
    "        print(f\"Encoders saved to: {config['encoders_path']}\")\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'feature_cols': artifacts['feature_cols'],\n",
    "            'categorical_cols': artifacts['categorical_cols'],\n",
    "            'metrics': artifacts['analysis_results']['metrics'],\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'python_version': sys.version,\n",
    "            'xgboost_version': xgb.__version__,\n",
    "            'sklearn_version': pd.__version__\n",
    "        }\n",
    "        \n",
    "        with open(config['metadata_path'], 'w') as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "        print(f\"Metadata saved to: {config['metadata_path']}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model artifacts: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Create artifacts dictionary\n",
    "artifacts = {\n",
    "    'model': model,\n",
    "    'encoders': encoders,\n",
    "    'feature_cols': feature_cols,\n",
    "    'categorical_cols': categorical_cols,\n",
    "    'cv_scores': cv_scores,\n",
    "    'analysis_results': analysis_results\n",
    "}\n",
    "\n",
    "# Save model and artifacts\n",
    "try:\n",
    "    # Build a deployment-style config expected by save_model_artifacts\n",
    "    try:\n",
    "        deploy_base = Path(config.get('paths', {}).get('deploy', Path('deployment')))\n",
    "    except Exception:\n",
    "        deploy_base = Path('deployment')\n",
    "\n",
    "    models_dir = deploy_base / 'models'\n",
    "    metadata_dir = deploy_base / 'config'\n",
    "\n",
    "    # Ensure directories exist\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    metadata_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Prepare config with the exact keys save_model_artifacts expects\n",
    "    save_config = {\n",
    "        'deploy_dir': deploy_base,\n",
    "        'model_path': models_dir / 'bid_fee_model.pkl',\n",
    "        'encoders_path': models_dir / 'encoders.pkl',\n",
    "        'metadata_path': metadata_dir / 'metadata.json'\n",
    "    }\n",
    "\n",
    "    # Call saver\n",
    "    success = save_model_artifacts(model, save_config, artifacts)\n",
    "    if success:\n",
    "        print(\"\\nModel and artifacts saved successfully!\")\n",
    "        print(\"\\nModel is ready for deployment with the following files:\")\n",
    "        # Print files and keep backward-compatible keys in the original config\n",
    "        print(f\"- Model: {save_config['model_path']}\")\n",
    "        print(f\"- Encoders: {save_config['encoders_path']}\")\n",
    "        print(f\"- Metadata: {save_config['metadata_path']}\")\n",
    "        # update original config dict so subsequent code referencing `config[...]` won't fail\n",
    "        if isinstance(config, dict):\n",
    "            config['model_path'] = save_config['model_path']\n",
    "            config['encoders_path'] = save_config['encoders_path']\n",
    "            config['metadata_path'] = save_config['metadata_path']\n",
    "        print(f\"- Encoders: {config['encoders_path']}\")\n",
    "        print(f\"- Metadata: {config['metadata_path']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to save model artifacts: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03de935a",
   "metadata": {},
   "source": [
    "# Bid Recommendation Model\n",
    "\n",
    "This notebook implements a machine learning pipeline for bid recommendations, including:\n",
    "1. Data loading and preprocessing\n",
    "2. Feature engineering\n",
    "3. Model training (regressor for bid amount, classifier for win probability)\n",
    "4. Model evaluation\n",
    "5. Production deployment helpers\n",
    "\n",
    "## Setup Instructions\n",
    "1. Install required packages (first cell)\n",
    "2. Optional: Set up FRED API key for economic indicators\n",
    "3. Run cells in order\n",
    "4. Models will be saved to `models/` directory\n",
    "\n",
    "## Notes\n",
    "- Uses time-series cross-validation\n",
    "- Handles missing values and outliers\n",
    "- Includes feature importance analysis\n",
    "- Provides production-ready inference class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6962cc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install and import dependencies\n",
    "!pip install xgboost scikit-learn pandas numpy matplotlib seaborn fredapi shap --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5422dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (156733, 51)\n",
      "Columns: ['BidId', 'BidFileNumber', 'BidName', 'BidDate', 'Bid_DueDate', 'BidFee', 'TargetTime', 'WinProbability', 'BidStatusName', 'Bid_JobPurpose', 'Bid_Deliverable', 'Market', 'Submarket', 'BusinessSegment', 'BusinessSegmentDetail', 'DistanceInMiles', 'Bid_Property_Type', 'PropertyId', 'OfficeId', 'PropertyName', 'PropertyType', 'SubType', 'PropertyCity', 'PropertyState', 'RooftopLongitude', 'RooftopLatitude', 'ZipCode', 'JobCount', 'IECount', 'LeaseCount', 'SaleCount', 'MarketOrientation', 'AddressDisplayCalc', 'GrossBuildingAreaRange', 'YearBuiltRange', 'OfficeCode', 'OfficeCompanyName', 'OfficeLocation', 'JobId', 'JobName', 'JobStatus', 'PopulationEstimate', 'AverageHouseValue', 'IncomePerHousehold', 'MedianAge', 'DeliveryTotal', 'NumberofBusinesses', 'NumberofEmployees', 'ZipPopulation', 'BidCompanyName', 'BidCompanyType']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BidId</th>\n",
       "      <th>BidFileNumber</th>\n",
       "      <th>BidName</th>\n",
       "      <th>BidDate</th>\n",
       "      <th>Bid_DueDate</th>\n",
       "      <th>BidFee</th>\n",
       "      <th>TargetTime</th>\n",
       "      <th>WinProbability</th>\n",
       "      <th>BidStatusName</th>\n",
       "      <th>Bid_JobPurpose</th>\n",
       "      <th>...</th>\n",
       "      <th>PopulationEstimate</th>\n",
       "      <th>AverageHouseValue</th>\n",
       "      <th>IncomePerHousehold</th>\n",
       "      <th>MedianAge</th>\n",
       "      <th>DeliveryTotal</th>\n",
       "      <th>NumberofBusinesses</th>\n",
       "      <th>NumberofEmployees</th>\n",
       "      <th>ZipPopulation</th>\n",
       "      <th>BidCompanyName</th>\n",
       "      <th>BidCompanyType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225469</td>\n",
       "      <td>B179-2025-000742</td>\n",
       "      <td>36 US-87</td>\n",
       "      <td>9/7/2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Active</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5225.0</td>\n",
       "      <td>285400.0</td>\n",
       "      <td>68767.0</td>\n",
       "      <td>48.700001</td>\n",
       "      <td>2970.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>1615.0</td>\n",
       "      <td>5868.0</td>\n",
       "      <td>Jefferson Bank</td>\n",
       "      <td>Banks / Credit Institutions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>225471</td>\n",
       "      <td>B185-2025-000244</td>\n",
       "      <td>Chase Bank Ground Lease</td>\n",
       "      <td>9/7/2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Active</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>24723.0</td>\n",
       "      <td>481100.0</td>\n",
       "      <td>87434.0</td>\n",
       "      <td>36.599998</td>\n",
       "      <td>12443.0</td>\n",
       "      <td>1086.0</td>\n",
       "      <td>12795.0</td>\n",
       "      <td>23847.0</td>\n",
       "      <td>Integra Realty Resources - Dallas</td>\n",
       "      <td>Professional Service (PS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>225470</td>\n",
       "      <td>B179-2025-000743</td>\n",
       "      <td>Winston Hills Shopping Center</td>\n",
       "      <td>9/7/2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Active</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>44678.0</td>\n",
       "      <td>159900.0</td>\n",
       "      <td>49050.0</td>\n",
       "      <td>36.099998</td>\n",
       "      <td>18961.0</td>\n",
       "      <td>771.0</td>\n",
       "      <td>13868.0</td>\n",
       "      <td>40589.0</td>\n",
       "      <td>Texas Partners Bank</td>\n",
       "      <td>Banks / Credit Institutions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>225383</td>\n",
       "      <td>B130-2025-000590</td>\n",
       "      <td>Vacant Residential Land</td>\n",
       "      <td>9/5/2025</td>\n",
       "      <td>9/15/2025</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Won</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3900.0</td>\n",
       "      <td>177400.0</td>\n",
       "      <td>63476.0</td>\n",
       "      <td>44.400002</td>\n",
       "      <td>2276.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>3834.0</td>\n",
       "      <td>One Florida Bank</td>\n",
       "      <td>Banks / Credit Institutions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>225385</td>\n",
       "      <td>B142-2025-000073</td>\n",
       "      <td>Somerset Apartments</td>\n",
       "      <td>9/5/2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Active</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>35902.0</td>\n",
       "      <td>94000.0</td>\n",
       "      <td>45280.0</td>\n",
       "      <td>34.400002</td>\n",
       "      <td>17280.0</td>\n",
       "      <td>543.0</td>\n",
       "      <td>12051.0</td>\n",
       "      <td>33702.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    BidId     BidFileNumber                        BidName   BidDate  \\\n",
       "0  225469  B179-2025-000742                       36 US-87  9/7/2025   \n",
       "1  225471  B185-2025-000244        Chase Bank Ground Lease  9/7/2025   \n",
       "2  225470  B179-2025-000743  Winston Hills Shopping Center  9/7/2025   \n",
       "3  225383  B130-2025-000590        Vacant Residential Land  9/5/2025   \n",
       "4  225385  B142-2025-000073            Somerset Apartments  9/5/2025   \n",
       "\n",
       "  Bid_DueDate  BidFee  TargetTime  WinProbability BidStatusName  \\\n",
       "0         NaN     NaN         NaN             NaN        Active   \n",
       "1         NaN     NaN         NaN             NaN        Active   \n",
       "2         NaN     NaN         NaN             NaN        Active   \n",
       "3   9/15/2025  3700.0        10.0             NaN           Won   \n",
       "4         NaN  4000.0        30.0             NaN        Active   \n",
       "\n",
       "  Bid_JobPurpose  ... PopulationEstimate AverageHouseValue IncomePerHousehold  \\\n",
       "0            NaN  ...             5225.0          285400.0            68767.0   \n",
       "1            NaN  ...            24723.0          481100.0            87434.0   \n",
       "2            NaN  ...            44678.0          159900.0            49050.0   \n",
       "3            NaN  ...             3900.0          177400.0            63476.0   \n",
       "4            NaN  ...            35902.0           94000.0            45280.0   \n",
       "\n",
       "   MedianAge  DeliveryTotal  NumberofBusinesses NumberofEmployees  \\\n",
       "0  48.700001         2970.0               153.0            1615.0   \n",
       "1  36.599998        12443.0              1086.0           12795.0   \n",
       "2  36.099998        18961.0               771.0           13868.0   \n",
       "3  44.400002         2276.0                70.0             299.0   \n",
       "4  34.400002        17280.0               543.0           12051.0   \n",
       "\n",
       "   ZipPopulation                     BidCompanyName  \\\n",
       "0         5868.0                     Jefferson Bank   \n",
       "1        23847.0  Integra Realty Resources - Dallas   \n",
       "2        40589.0                Texas Partners Bank   \n",
       "3         3834.0                   One Florida Bank   \n",
       "4        33702.0                                NaN   \n",
       "\n",
       "                BidCompanyType  \n",
       "0  Banks / Credit Institutions  \n",
       "1    Professional Service (PS)  \n",
       "2  Banks / Credit Institutions  \n",
       "3  Banks / Credit Institutions  \n",
       "4                          NaN  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2: Load CSV\n",
    "csv_path = r\"D:\\Global stat solutions\\GSS\\Bid Recommendation System\\Prerequisites\\bidrecommendation.csv\"  # <-- yahan apne dataset ka naam daal\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50f86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2018-01-02 00:00:00 to 2025-09-07 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BidDate</th>\n",
       "      <th>BidWeek</th>\n",
       "      <th>BidMonth</th>\n",
       "      <th>BidYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-07</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-07</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-07</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     BidDate  BidWeek  BidMonth  BidYear\n",
       "0 2025-09-07       36         9     2025\n",
       "1 2025-09-07       36         9     2025\n",
       "2 2025-09-07       36         9     2025\n",
       "3 2025-09-05       36         9     2025\n",
       "4 2025-09-05       36         9     2025"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3: Convert BidDate and extract time features\n",
    "df['BidDate'] = pd.to_datetime(df['BidDate'], errors='coerce')\n",
    "df = df.dropna(subset=['BidDate']).reset_index(drop=True)\n",
    "\n",
    "df['BidWeek'] = df['BidDate'].dt.isocalendar().week\n",
    "df['BidMonth'] = df['BidDate'].dt.month\n",
    "df['BidYear'] = df['BidDate'].dt.year\n",
    "df['BidWeekStart'] = df['BidDate'].dt.to_period('W').apply(lambda r: r.start_time)\n",
    "\n",
    "print(\"Date range:\", df['BidDate'].min(), \"to\", df['BidDate'].max())\n",
    "df[['BidDate','BidWeek','BidMonth','BidYear']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800b05ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly agg shape: (68067, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ZipCode</th>\n",
       "      <th>BidWeekStart</th>\n",
       "      <th>median_BidFee</th>\n",
       "      <th>week_win_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-02-12</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-02-19</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-02-26</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-26</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-09</td>\n",
       "      <td>1612.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ZipCode BidWeekStart  median_BidFee  week_win_rate\n",
       "0       0   2018-02-12         4200.0            NaN\n",
       "1       0   2018-02-19         4800.0            NaN\n",
       "2       0   2018-02-26         5000.0            NaN\n",
       "3       0   2018-03-26         3250.0            NaN\n",
       "4       0   2018-04-09         1612.5            NaN"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Aggregate median BidFee per ZIP-week\n",
    "agg = df.groupby(['ZipCode','BidWeekStart']).agg(\n",
    "    median_BidFee=('BidFee','median'),\n",
    "    week_win_rate=('WinProbability','mean')\n",
    ").reset_index()\n",
    "\n",
    "print(\"Weekly agg shape:\", agg.shape)\n",
    "agg.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab8ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After lag creation: (48470, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ZipCode</th>\n",
       "      <th>BidWeekStart</th>\n",
       "      <th>median_BidFee</th>\n",
       "      <th>week_win_rate</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_2</th>\n",
       "      <th>lag_3</th>\n",
       "      <th>lag_4</th>\n",
       "      <th>roll_4w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-26</td>\n",
       "      <td>3250.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>4800.00</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4666.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-09</td>\n",
       "      <td>1612.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3250.00</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>4312.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-21</td>\n",
       "      <td>4668.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1612.50</td>\n",
       "      <td>3250.00</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>3665.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-07-09</td>\n",
       "      <td>3500.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4668.75</td>\n",
       "      <td>1612.50</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>3632.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-07-30</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3500.00</td>\n",
       "      <td>4668.75</td>\n",
       "      <td>1612.5</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>3257.812500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ZipCode BidWeekStart  median_BidFee  week_win_rate    lag_1    lag_2  \\\n",
       "3       0   2018-03-26        3250.00            NaN  5000.00  4800.00   \n",
       "4       0   2018-04-09        1612.50            NaN  3250.00  5000.00   \n",
       "5       0   2018-05-21        4668.75            NaN  1612.50  3250.00   \n",
       "6       0   2018-07-09        3500.00            NaN  4668.75  1612.50   \n",
       "7       0   2018-07-30        2000.00            NaN  3500.00  4668.75   \n",
       "\n",
       "    lag_3   lag_4      roll_4w  \n",
       "3  4200.0     NaN  4666.666667  \n",
       "4  4800.0  4200.0  4312.500000  \n",
       "5  5000.0  4800.0  3665.625000  \n",
       "6  3250.0  5000.0  3632.812500  \n",
       "7  1612.5  3250.0  3257.812500  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 5: Lag and Rolling Features\n",
    "for lag in [1,2,3,4]:\n",
    "    agg[f'lag_{lag}'] = agg.groupby('ZipCode')['median_BidFee'].shift(lag)\n",
    "\n",
    "agg['roll_4w'] = (agg.groupby('ZipCode')['median_BidFee']\n",
    "                  .shift(1).rolling(window=4,min_periods=1).mean()\n",
    "                  .reset_index(level=0,drop=True))\n",
    "\n",
    "agg = agg.dropna(subset=['lag_1','lag_2','lag_3'])\n",
    "print(\"After lag creation:\", agg.shape)\n",
    "agg.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ac2af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature engineering...\n",
      "\n",
      "Cleaning target variable...\n",
      "Initial BidFee range: $0.00 to $100000000.00\n",
      "NaN values in BidFee: 2861\n",
      "Infinite values in BidFee: 0\n",
      "\n",
      "After cleaning:\n",
      "BidFee range: $0.00 to $15000.00\n",
      "Number of rows: 152447\n",
      "\n",
      "Available columns: BidId, BidFileNumber, BidName, BidDate, Bid_DueDate, BidFee, TargetTime, WinProbability, BidStatusName, Bid_JobPurpose, Bid_Deliverable, Market, Submarket, BusinessSegment, BusinessSegmentDetail, DistanceInMiles, Bid_Property_Type, PropertyId, OfficeId, PropertyName, PropertyType, SubType, PropertyCity, PropertyState, RooftopLongitude, RooftopLatitude, ZipCode, JobCount, IECount, LeaseCount, SaleCount, MarketOrientation, AddressDisplayCalc, GrossBuildingAreaRange, YearBuiltRange, OfficeCode, OfficeCompanyName, OfficeLocation, JobId, JobName, JobStatus, PopulationEstimate, AverageHouseValue, IncomePerHousehold, MedianAge, DeliveryTotal, NumberofBusinesses, NumberofEmployees, ZipPopulation, BidCompanyName, BidCompanyType, BidWeek, BidMonth, BidYear, BidWeekStart\n",
      "\n",
      "Encoding categorical columns...\n",
      "Processing categorical columns: ['ZipCode', 'PropertyType', 'SubType', 'PropertyState', 'Market', 'Submarket', 'BusinessSegment', 'BidCompanyType']\n",
      "\n",
      "After cleaning:\n",
      "BidFee range: $0.00 to $15000.00\n",
      "Number of rows: 152447\n",
      "\n",
      "Available columns: BidId, BidFileNumber, BidName, BidDate, Bid_DueDate, BidFee, TargetTime, WinProbability, BidStatusName, Bid_JobPurpose, Bid_Deliverable, Market, Submarket, BusinessSegment, BusinessSegmentDetail, DistanceInMiles, Bid_Property_Type, PropertyId, OfficeId, PropertyName, PropertyType, SubType, PropertyCity, PropertyState, RooftopLongitude, RooftopLatitude, ZipCode, JobCount, IECount, LeaseCount, SaleCount, MarketOrientation, AddressDisplayCalc, GrossBuildingAreaRange, YearBuiltRange, OfficeCode, OfficeCompanyName, OfficeLocation, JobId, JobName, JobStatus, PopulationEstimate, AverageHouseValue, IncomePerHousehold, MedianAge, DeliveryTotal, NumberofBusinesses, NumberofEmployees, ZipPopulation, BidCompanyName, BidCompanyType, BidWeek, BidMonth, BidYear, BidWeekStart\n",
      "\n",
      "Encoding categorical columns...\n",
      "Processing categorical columns: ['ZipCode', 'PropertyType', 'SubType', 'PropertyState', 'Market', 'Submarket', 'BusinessSegment', 'BidCompanyType']\n",
      "\n",
      "Creating time-based features...\n",
      "\n",
      "Calculating market-based features...\n",
      "\n",
      "Creating time-based features...\n",
      "\n",
      "Calculating market-based features...\n",
      "\n",
      "Calculating rolling statistics...\n",
      "\n",
      "Calculating rolling statistics...\n",
      "\n",
      "Creating business and property features...\n",
      "\n",
      "Handling missing values...\n",
      "\n",
      "Creating business and property features...\n",
      "\n",
      "Handling missing values...\n",
      "\n",
      "Preparing feature matrix...\n",
      "\n",
      "Available features: ZipCode_encoded, PropertyType_encoded, SubType_encoded, PropertyState_encoded, Market_encoded, Submarket_encoded, BusinessSegment_encoded, BidCompanyType_encoded, Year, Month, Week, DayOfWeek, PopulationEstimate_zip_ratio, AverageHouseValue_zip_ratio, IncomePerHousehold_zip_ratio, MedianAge_zip_ratio, NumberofBusinesses_zip_ratio, NumberofEmployees_zip_ratio, ZipPopulation_zip_ratio, rolling_7d_mean_fee, rolling_7d_median_fee, rolling_7d_win_rate, rolling_30d_mean_fee, rolling_30d_median_fee, rolling_30d_win_rate, rolling_90d_mean_fee, rolling_90d_median_fee, rolling_90d_win_rate, company_type_freq, property_type_freq, DistanceInMiles, JobCount, IECount, LeaseCount, SaleCount, PopulationEstimate, AverageHouseValue, IncomePerHousehold, MedianAge, NumberofBusinesses, NumberofEmployees\n",
      "\n",
      "Feature matrix shape: (152447, 41)\n",
      "\n",
      "Preparing target variables...\n",
      "Regression target: BidFee\n",
      "Target range: $0.00 to $15000.00\n",
      "Classification target available\n",
      "\n",
      "Feature engineering complete.\n",
      "\n",
      "Preparing feature matrix...\n",
      "\n",
      "Available features: ZipCode_encoded, PropertyType_encoded, SubType_encoded, PropertyState_encoded, Market_encoded, Submarket_encoded, BusinessSegment_encoded, BidCompanyType_encoded, Year, Month, Week, DayOfWeek, PopulationEstimate_zip_ratio, AverageHouseValue_zip_ratio, IncomePerHousehold_zip_ratio, MedianAge_zip_ratio, NumberofBusinesses_zip_ratio, NumberofEmployees_zip_ratio, ZipPopulation_zip_ratio, rolling_7d_mean_fee, rolling_7d_median_fee, rolling_7d_win_rate, rolling_30d_mean_fee, rolling_30d_median_fee, rolling_30d_win_rate, rolling_90d_mean_fee, rolling_90d_median_fee, rolling_90d_win_rate, company_type_freq, property_type_freq, DistanceInMiles, JobCount, IECount, LeaseCount, SaleCount, PopulationEstimate, AverageHouseValue, IncomePerHousehold, MedianAge, NumberofBusinesses, NumberofEmployees\n",
      "\n",
      "Feature matrix shape: (152447, 41)\n",
      "\n",
      "Preparing target variables...\n",
      "Regression target: BidFee\n",
      "Target range: $0.00 to $15000.00\n",
      "Classification target available\n",
      "\n",
      "Feature engineering complete.\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "print(\"Starting feature engineering...\")\n",
    "\n",
    "# 0. Clean target variable\n",
    "print(\"\\nCleaning target variable...\")\n",
    "if 'BidFee' in df.columns:\n",
    "    print(f\"Initial BidFee range: ${df['BidFee'].min():.2f} to ${df['BidFee'].max():.2f}\")\n",
    "    print(f\"NaN values in BidFee: {df['BidFee'].isna().sum()}\")\n",
    "    print(f\"Infinite values in BidFee: {np.isinf(df['BidFee']).sum()}\")\n",
    "    \n",
    "    # Remove rows with invalid BidFee\n",
    "    df = df[~df['BidFee'].isna()]\n",
    "    df = df[~np.isinf(df['BidFee'])]\n",
    "    df = df[df['BidFee'] >= 0]  # Remove negative fees\n",
    "    df = df[df['BidFee'] <= df['BidFee'].quantile(0.99)]  # Remove extreme outliers\n",
    "    \n",
    "    print(f\"\\nAfter cleaning:\")\n",
    "    print(f\"BidFee range: ${df['BidFee'].min():.2f} to ${df['BidFee'].max():.2f}\")\n",
    "    print(f\"Number of rows: {len(df)}\")\n",
    "\n",
    "print(\"\\nAvailable columns:\", ', '.join(df.columns))\n",
    "\n",
    "# 1. Handle categorical columns\n",
    "print(\"\\nEncoding categorical columns...\")\n",
    "categorical_cols = [\n",
    "    'ZipCode', \n",
    "    'PropertyType',\n",
    "    'SubType',\n",
    "    'PropertyState',\n",
    "    'Market',\n",
    "    'Submarket',\n",
    "    'BusinessSegment',\n",
    "    'BidCompanyType'\n",
    "]\n",
    "cat_cols = [col for col in categorical_cols if col in df.columns]\n",
    "print(f\"Processing categorical columns: {cat_cols}\")\n",
    "\n",
    "encoders = {}\n",
    "for col in cat_cols:\n",
    "    if col in df.columns:\n",
    "        encoders[col] = LabelEncoder()\n",
    "        df[f'{col}_encoded'] = encoders[col].fit_transform(df[col].astype(str).fillna('MISSING'))\n",
    "\n",
    "# 2. Create time-based features\n",
    "print(\"\\nCreating time-based features...\")\n",
    "if 'BidDate' in df.columns:\n",
    "    df['BidDate'] = pd.to_datetime(df['BidDate'])\n",
    "    df['Year'] = df['BidDate'].dt.year\n",
    "    df['Month'] = df['BidDate'].dt.month\n",
    "    df['Week'] = df['BidDate'].dt.isocalendar().week\n",
    "    df['DayOfWeek'] = df['BidDate'].dt.dayofweek\n",
    "\n",
    "# 3. Calculate market-based features\n",
    "print(\"\\nCalculating market-based features...\")\n",
    "market_cols = [\n",
    "    'PopulationEstimate',\n",
    "    'AverageHouseValue',\n",
    "    'IncomePerHousehold',\n",
    "    'MedianAge',\n",
    "    'NumberofBusinesses',\n",
    "    'NumberofEmployees',\n",
    "    'ZipPopulation'\n",
    "]\n",
    "\n",
    "for col in market_cols:\n",
    "    if col in df.columns:\n",
    "        # Calculate ZIP-wise statistics\n",
    "        df[f'{col}_zip_ratio'] = df[col] / df.groupby('ZipCode')[col].transform('mean')\n",
    "\n",
    "# 4. Calculate rolling statistics by ZIP code\n",
    "print(\"\\nCalculating rolling statistics...\")\n",
    "if 'BidDate' in df.columns and 'ZipCode' in df.columns:\n",
    "    df = df.sort_values(['ZipCode', 'BidDate'])\n",
    "    \n",
    "    for window in [7, 30, 90]:  # Days-based windows\n",
    "        # Bid fee statistics\n",
    "        if 'BidFee' in df.columns:\n",
    "            df[f'rolling_{window}d_mean_fee'] = df.groupby('ZipCode')['BidFee'].transform(\n",
    "                lambda x: x.rolling(window, min_periods=1).mean()\n",
    "            )\n",
    "            df[f'rolling_{window}d_median_fee'] = df.groupby('ZipCode')['BidFee'].transform(\n",
    "                lambda x: x.rolling(window, min_periods=1).median()\n",
    "            )\n",
    "        \n",
    "        # Win probability statistics if available\n",
    "        if 'WinProbability' in df.columns:\n",
    "            df[f'rolling_{window}d_win_rate'] = df.groupby('ZipCode')['WinProbability'].transform(\n",
    "                lambda x: x.rolling(window, min_periods=1).mean()\n",
    "            )\n",
    "\n",
    "# 5. Business type and property features\n",
    "print(\"\\nCreating business and property features...\")\n",
    "if 'BidCompanyType' in df.columns:\n",
    "    # Company type frequency encoding\n",
    "    type_freq = df['BidCompanyType'].value_counts(normalize=True).to_dict()\n",
    "    df['company_type_freq'] = df['BidCompanyType'].map(type_freq)\n",
    "\n",
    "if 'PropertyType' in df.columns:\n",
    "    # Property type frequency encoding\n",
    "    prop_freq = df['PropertyType'].value_counts(normalize=True).to_dict()\n",
    "    df['property_type_freq'] = df['PropertyType'].map(prop_freq)\n",
    "\n",
    "# 6. Distance and location features\n",
    "if 'DistanceInMiles' in df.columns:\n",
    "    df['distance_bucket'] = pd.qcut(df['DistanceInMiles'], q=5, labels=['VeryClose', 'Close', 'Medium', 'Far', 'VeryFar'])\n",
    "    df['distance_bucket_encoded'] = LabelEncoder().fit_transform(df['distance_bucket'])\n",
    "\n",
    "# 7. Handle missing values\n",
    "print(\"\\nHandling missing values...\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if df[col].isna().any():\n",
    "        # Fill with median by group if possible\n",
    "        try:\n",
    "            df[col] = df.groupby('ZipCode')[col].transform(\n",
    "                lambda x: x.fillna(x.median())\n",
    "            )\n",
    "        except:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# 8. Prepare feature matrix\n",
    "print(\"\\nPreparing feature matrix...\")\n",
    "base_features = [\n",
    "    # Encoded categoricals\n",
    "    *[f'{col}_encoded' for col in cat_cols],\n",
    "    \n",
    "    # Time features\n",
    "    'Year', 'Month', 'Week', 'DayOfWeek',\n",
    "    \n",
    "    # Market features\n",
    "    *[f'{col}_zip_ratio' for col in market_cols if f'{col}_zip_ratio' in df.columns],\n",
    "    \n",
    "    # Rolling stats\n",
    "    *[col for col in df.columns if col.startswith('rolling_')],\n",
    "    \n",
    "    # Business and property features\n",
    "    'company_type_freq', 'property_type_freq',\n",
    "    \n",
    "    # Original numeric features\n",
    "    'DistanceInMiles', 'JobCount', 'IECount', 'LeaseCount', 'SaleCount',\n",
    "    'PopulationEstimate', 'AverageHouseValue', 'IncomePerHousehold',\n",
    "    'MedianAge', 'NumberofBusinesses', 'NumberofEmployees'\n",
    "]\n",
    "\n",
    "# Only use features that exist\n",
    "feature_cols = [col for col in base_features if col in df.columns]\n",
    "print(\"\\nAvailable features:\", ', '.join(feature_cols))\n",
    "\n",
    "# Create feature matrix\n",
    "X = df[feature_cols].copy()\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "\n",
    "# 9. Prepare target variables\n",
    "print(\"\\nPreparing target variables...\")\n",
    "if 'BidFee' in df.columns:\n",
    "    target_reg = 'BidFee'\n",
    "    y_reg = df[target_reg]\n",
    "    print(f\"Regression target: {target_reg}\")\n",
    "    print(f\"Target range: ${y_reg.min():.2f} to ${y_reg.max():.2f}\")\n",
    "else:\n",
    "    raise ValueError(\"BidFee column not found - required for training!\")\n",
    "\n",
    "# Classifier target if available\n",
    "if 'WinProbability' in df.columns:\n",
    "    y_clf = (df['WinProbability'] > df['WinProbability'].median()).astype(int)\n",
    "    print(\"Classification target available\")\n",
    "else:\n",
    "    y_clf = None\n",
    "    print(\"No classification target available\")\n",
    "\n",
    "# Save variables to global scope\n",
    "globals().update({\n",
    "    'X': X,\n",
    "    'y_reg': y_reg,\n",
    "    'y_clf': y_clf,\n",
    "    'target_reg': target_reg,\n",
    "    'feature_cols': feature_cols,\n",
    "    'encoders': encoders,\n",
    "    'cat_cols': cat_cols\n",
    "})\n",
    "\n",
    "print(\"\\nFeature engineering complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4326ef94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "\n",
      "Validating input data...\n",
      "Warning: NaN values found in features. Filling with 0...\n",
      "Training with 152447 samples and 41 features\n",
      "Target range: $0.00 to $15000.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing cross-validation...\n",
      "\n",
      "Fold 1 Results:\n",
      "RMSE: $1341.86\n",
      "MAE: $826.27\n",
      "R2 Score: 0.470\n",
      "\n",
      "Fold 1 Results:\n",
      "RMSE: $1341.86\n",
      "MAE: $826.27\n",
      "R2 Score: 0.470\n",
      "\n",
      "Fold 2 Results:\n",
      "RMSE: $1415.53\n",
      "MAE: $903.73\n",
      "R2 Score: 0.416\n",
      "\n",
      "Fold 2 Results:\n",
      "RMSE: $1415.53\n",
      "MAE: $903.73\n",
      "R2 Score: 0.416\n",
      "\n",
      "Fold 3 Results:\n",
      "RMSE: $1475.95\n",
      "MAE: $961.08\n",
      "R2 Score: 0.377\n",
      "\n",
      "Fold 3 Results:\n",
      "RMSE: $1475.95\n",
      "MAE: $961.08\n",
      "R2 Score: 0.377\n",
      "\n",
      "Fold 4 Results:\n",
      "RMSE: $1378.92\n",
      "MAE: $845.23\n",
      "R2 Score: 0.422\n",
      "\n",
      "Fold 4 Results:\n",
      "RMSE: $1378.92\n",
      "MAE: $845.23\n",
      "R2 Score: 0.422\n",
      "\n",
      "Fold 5 Results:\n",
      "RMSE: $1372.13\n",
      "MAE: $841.97\n",
      "R2 Score: 0.437\n",
      "\n",
      "Cross-validation Average Results:\n",
      "Average RMSE: $1396.88 ± $45.96\n",
      "Average MAE: $875.66 ± $50.17\n",
      "Average R2: 0.424 ± 0.030\n",
      "\n",
      "Training final model on full dataset...\n",
      "\n",
      "Fold 5 Results:\n",
      "RMSE: $1372.13\n",
      "MAE: $841.97\n",
      "R2 Score: 0.437\n",
      "\n",
      "Cross-validation Average Results:\n",
      "Average RMSE: $1396.88 ± $45.96\n",
      "Average MAE: $875.66 ± $50.17\n",
      "Average R2: 0.424 ± 0.030\n",
      "\n",
      "Training final model on full dataset...\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                         feature  importance\n",
      "19           rolling_7d_mean_fee    0.267992\n",
      "1           PropertyType_encoded    0.072396\n",
      "3          PropertyState_encoded    0.060804\n",
      "22          rolling_30d_mean_fee    0.048274\n",
      "12  PopulationEstimate_zip_ratio    0.037704\n",
      "25          rolling_90d_mean_fee    0.029678\n",
      "30               DistanceInMiles    0.029304\n",
      "6        BusinessSegment_encoded    0.029273\n",
      "16  NumberofBusinesses_zip_ratio    0.028602\n",
      "17   NumberofEmployees_zip_ratio    0.027932\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                         feature  importance\n",
      "19           rolling_7d_mean_fee    0.267992\n",
      "1           PropertyType_encoded    0.072396\n",
      "3          PropertyState_encoded    0.060804\n",
      "22          rolling_30d_mean_fee    0.048274\n",
      "12  PopulationEstimate_zip_ratio    0.037704\n",
      "25          rolling_90d_mean_fee    0.029678\n",
      "30               DistanceInMiles    0.029304\n",
      "6        BusinessSegment_encoded    0.029273\n",
      "16  NumberofBusinesses_zip_ratio    0.028602\n",
      "17   NumberofEmployees_zip_ratio    0.027932\n",
      "\n",
      "Error Distribution:\n",
      "25th percentile: $206.95\n",
      "Median error: $492.16\n",
      "75th percentile: $989.26\n",
      "90th percentile: $1767.25\n",
      "95th percentile: $2511.03\n",
      "99th percentile: $4726.30\n",
      "\n",
      "Prediction Calibration:\n",
      "                        predicted              actual\n",
      "                             mean  count         mean\n",
      "(-0.001, 1980.038]    1488.954590  15245  1382.887571\n",
      "(1980.038, 2401.773]  2209.213867  15245  2133.854408\n",
      "(2401.773, 2705.877]  2560.463379  15244  2506.397374\n",
      "(2705.877, 2931.404]  2821.207764  15245  2759.154772\n",
      "(2931.404, 3181.349]  3058.450195  15253  3016.444826\n",
      "(3181.349, 3401.558]  3286.806396  15236  3261.627739\n",
      "(3401.558, 3633.829]  3513.137207  15246  3500.879685\n",
      "(3633.829, 3948.83]   3783.678467  15243  3823.811124\n",
      "(3948.83, 4544.595]   4196.687500  15245  4269.913294\n",
      "(4544.595, 16314.28]  5863.535645  15245  6127.314226\n",
      "\n",
      "Model training completed successfully!\n",
      "\n",
      "Error Distribution:\n",
      "25th percentile: $206.95\n",
      "Median error: $492.16\n",
      "75th percentile: $989.26\n",
      "90th percentile: $1767.25\n",
      "95th percentile: $2511.03\n",
      "99th percentile: $4726.30\n",
      "\n",
      "Prediction Calibration:\n",
      "                        predicted              actual\n",
      "                             mean  count         mean\n",
      "(-0.001, 1980.038]    1488.954590  15245  1382.887571\n",
      "(1980.038, 2401.773]  2209.213867  15245  2133.854408\n",
      "(2401.773, 2705.877]  2560.463379  15244  2506.397374\n",
      "(2705.877, 2931.404]  2821.207764  15245  2759.154772\n",
      "(2931.404, 3181.349]  3058.450195  15253  3016.444826\n",
      "(3181.349, 3401.558]  3286.806396  15236  3261.627739\n",
      "(3401.558, 3633.829]  3513.137207  15246  3500.879685\n",
      "(3633.829, 3948.83]   3783.678467  15243  3823.811124\n",
      "(3948.83, 4544.595]   4196.687500  15245  4269.913294\n",
      "(4544.595, 16314.28]  5863.535645  15245  6127.314226\n",
      "\n",
      "Model training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Model Training and Validation\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "\n",
    "# 0. Data validation\n",
    "print(\"\\nValidating input data...\")\n",
    "if len(X) == 0:\n",
    "    raise ValueError(\"Empty feature matrix\")\n",
    "if len(y_reg) == 0:\n",
    "    raise ValueError(\"Empty target variable\")\n",
    "if X.isna().any().any():\n",
    "    print(\"Warning: NaN values found in features. Filling with 0...\")\n",
    "    X = X.fillna(0)\n",
    "if y_reg.isna().any():\n",
    "    print(\"Warning: NaN values found in target. Removing...\")\n",
    "    valid_idx = ~y_reg.isna()\n",
    "    X = X.loc[valid_idx]\n",
    "    y_reg = y_reg.loc[valid_idx]\n",
    "\n",
    "print(f\"Training with {len(X)} samples and {X.shape[1]} features\")\n",
    "print(f\"Target range: ${y_reg.min():.2f} to ${y_reg.max():.2f}\")\n",
    "\n",
    "# 1. Define time-based cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "cv_scores = {\n",
    "    'rmse': [],\n",
    "    'mae': [],\n",
    "    'r2': []\n",
    "}\n",
    "\n",
    "# 2. Initialize model with robust parameters\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    min_child_weight=2,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Sort data by date for time-based splitting\n",
    "if 'BidDate' in df.columns:\n",
    "    sort_idx = df['BidDate'].sort_values().index\n",
    "    X = X.loc[sort_idx]\n",
    "    y_reg = y_reg.loc[sort_idx]\n",
    "\n",
    "print(\"\\nPerforming cross-validation...\")\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):\n",
    "    try:\n",
    "        # Split data\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y_reg.iloc[train_idx], y_reg.iloc[test_idx]\n",
    "        \n",
    "        # Ensure positive target values\n",
    "        y_train = y_train.clip(lower=0)\n",
    "        y_test = y_test.clip(lower=0)\n",
    "        \n",
    "        # Train model\n",
    "        xgb_reg.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = xgb_reg.predict(X_test)\n",
    "        \n",
    "        # Ensure non-negative predictions\n",
    "        y_pred = np.maximum(y_pred, 0)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        cv_scores['rmse'].append(rmse)\n",
    "        cv_scores['mae'].append(mae)\n",
    "        cv_scores['r2'].append(r2)\n",
    "        \n",
    "        print(f\"\\nFold {fold} Results:\")\n",
    "        print(f\"RMSE: ${rmse:.2f}\")\n",
    "        print(f\"MAE: ${mae:.2f}\")\n",
    "        print(f\"R2 Score: {r2:.3f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in fold {fold}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "if len(cv_scores['rmse']) == 0:\n",
    "    raise ValueError(\"All cross-validation folds failed\")\n",
    "\n",
    "# Print average cross-validation scores\n",
    "print(\"\\nCross-validation Average Results:\")\n",
    "print(f\"Average RMSE: ${np.mean(cv_scores['rmse']):.2f} ± ${np.std(cv_scores['rmse']):.2f}\")\n",
    "print(f\"Average MAE: ${np.mean(cv_scores['mae']):.2f} ± ${np.std(cv_scores['mae']):.2f}\")\n",
    "print(f\"Average R2: {np.mean(cv_scores['r2']):.3f} ± {np.std(cv_scores['r2']):.3f}\")\n",
    "\n",
    "# Train final model on full dataset\n",
    "print(\"\\nTraining final model on full dataset...\")\n",
    "try:\n",
    "    # Ensure positive target values\n",
    "    y_reg_clean = y_reg.clip(lower=0)\n",
    "    \n",
    "    # Train final model\n",
    "    xgb_reg.fit(\n",
    "        X, y_reg_clean,\n",
    "        eval_set=[(X, y_reg_clean)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Feature importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': xgb_reg.feature_importances_\n",
    "    })\n",
    "    importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(importance_df.head(10))\n",
    "    \n",
    "    # Calculate error distribution\n",
    "    y_pred_full = xgb_reg.predict(X)\n",
    "    y_pred_full = np.maximum(y_pred_full, 0)  # Ensure non-negative predictions\n",
    "    errors = y_reg_clean - y_pred_full\n",
    "    error_percentiles = np.percentile(np.abs(errors), [25, 50, 75, 90, 95, 99])\n",
    "    \n",
    "    print(\"\\nError Distribution:\")\n",
    "    print(f\"25th percentile: ${error_percentiles[0]:.2f}\")\n",
    "    print(f\"Median error: ${error_percentiles[1]:.2f}\")\n",
    "    print(f\"75th percentile: ${error_percentiles[2]:.2f}\")\n",
    "    print(f\"90th percentile: ${error_percentiles[3]:.2f}\")\n",
    "    print(f\"95th percentile: ${error_percentiles[4]:.2f}\")\n",
    "    print(f\"99th percentile: ${error_percentiles[5]:.2f}\")\n",
    "    \n",
    "    # Calculate prediction calibration\n",
    "    pred_ranges = pd.qcut(y_pred_full, q=10)\n",
    "    calibration_df = pd.DataFrame({\n",
    "        'predicted': y_pred_full,\n",
    "        'actual': y_reg_clean\n",
    "    }).groupby(pred_ranges).agg({\n",
    "        'predicted': ['mean', 'count'],\n",
    "        'actual': 'mean'\n",
    "    })\n",
    "    \n",
    "    print(\"\\nPrediction Calibration:\")\n",
    "    print(calibration_df)\n",
    "    \n",
    "    # Save model artifacts\n",
    "    artifacts = {\n",
    "        'model': xgb_reg,\n",
    "        'feature_cols': feature_cols,\n",
    "        'encoders': encoders,\n",
    "        'cat_cols': cat_cols,\n",
    "        'cv_scores': cv_scores,\n",
    "        'feature_importance': importance_df,\n",
    "        'error_percentiles': error_percentiles,\n",
    "        'preprocessing_params': {\n",
    "            'categorical_cols': categorical_cols,\n",
    "            'market_cols': market_cols\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save variables to global scope\n",
    "    globals().update({\n",
    "        'xgb_reg': xgb_reg,\n",
    "        'artifacts': artifacts,\n",
    "        'importance_df': importance_df,\n",
    "        'cv_scores': cv_scores,\n",
    "        'error_percentiles': error_percentiles\n",
    "    })\n",
    "    \n",
    "    print(\"\\nModel training completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error training final model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd6a98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up production deployment...\n",
      "\n",
      "Saving model artifacts...\n",
      "Model saved to deployment\\models\\bid_fee_model.joblib\n",
      "Model configuration saved to deployment\\config\\model_config.json\n",
      "Encoders saved to deployment\\models\\encoders.joblib\n",
      "\n",
      "Creating prediction class...\n",
      "\n",
      "Testing prediction class...\n",
      "\n",
      "Test prediction successful!\n",
      "Predicted bid fee: $2678.39\n",
      "Confidence score: 80.00%\n",
      "90% Prediction interval: $911.13 to $4445.64\n",
      "\n",
      "Test prediction successful!\n",
      "Predicted bid fee: $2678.39\n",
      "Confidence score: 80.00%\n",
      "90% Prediction interval: $911.13 to $4445.64\n"
     ]
    }
   ],
   "source": [
    "# Production Deployment Setup\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Create deployment directories\n",
    "deploy_dir = Path('deployment')\n",
    "model_dir = deploy_dir / 'models'\n",
    "config_dir = deploy_dir / 'config'\n",
    "\n",
    "for d in [deploy_dir, model_dir, config_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Setting up production deployment...\")\n",
    "\n",
    "# 1. Save model and artifacts\n",
    "print(\"\\nSaving model artifacts...\")\n",
    "\n",
    "# Save main model\n",
    "model_path = model_dir / 'bid_fee_model.joblib'\n",
    "joblib.dump(artifacts['model'], model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Save encoders and configuration\n",
    "config = {\n",
    "    'feature_cols': artifacts['feature_cols'],\n",
    "    'categorical_cols': artifacts['preprocessing_params']['categorical_cols'],\n",
    "    'market_cols': artifacts['preprocessing_params']['market_cols'],\n",
    "    'model_info': {\n",
    "        'type': type(artifacts['model']).__name__,\n",
    "        'version': '1.0.0',\n",
    "        'created_at': datetime.datetime.now().isoformat(),\n",
    "        'python_version': platform.python_version(),\n",
    "        'package_versions': {\n",
    "            'xgboost': xgb.__version__,\n",
    "            'scikit-learn': sklearn.__version__,\n",
    "            'pandas': pd.__version__,\n",
    "            'numpy': np.__version__\n",
    "        }\n",
    "    },\n",
    "    'performance': {\n",
    "        'cv_scores': {\n",
    "            'rmse_mean': float(np.mean(artifacts['cv_scores']['rmse'])),\n",
    "            'rmse_std': float(np.std(artifacts['cv_scores']['rmse'])),\n",
    "            'mae_mean': float(np.mean(artifacts['cv_scores']['mae'])),\n",
    "            'mae_std': float(np.std(artifacts['cv_scores']['mae'])),\n",
    "            'r2_mean': float(np.mean(artifacts['cv_scores']['r2'])),\n",
    "            'r2_std': float(np.std(artifacts['cv_scores']['r2']))\n",
    "        },\n",
    "        'error_percentiles': {\n",
    "            'p25': float(artifacts['error_percentiles'][0]),\n",
    "            'p50': float(artifacts['error_percentiles'][1]),\n",
    "            'p75': float(artifacts['error_percentiles'][2]),\n",
    "            'p90': float(artifacts['error_percentiles'][3]),\n",
    "            'p95': float(artifacts['error_percentiles'][4]),\n",
    "            'p99': float(artifacts['error_percentiles'][5])\n",
    "        }\n",
    "    },\n",
    "    'feature_importance': artifacts['feature_importance'].to_dict(orient='records')[:10]  # Top 10 features\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_path = config_dir / 'model_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f\"Model configuration saved to {config_path}\")\n",
    "\n",
    "# Save encoders\n",
    "encoders_path = model_dir / 'encoders.joblib'\n",
    "joblib.dump(artifacts['encoders'], encoders_path)\n",
    "print(f\"Encoders saved to {encoders_path}\")\n",
    "\n",
    "# 2. Create prediction class for production\n",
    "print(\"\\nCreating prediction class...\")\n",
    "\n",
    "class BidPrediction:\n",
    "    \"\"\"Production-ready bid fee prediction class\"\"\"\n",
    "    \n",
    "    def __init__(self, model_dir: str = 'models'):\n",
    "        self.model_dir = Path(model_dir)\n",
    "        self._load_artifacts()\n",
    "        \n",
    "    def _load_artifacts(self):\n",
    "        \"\"\"Load model and related artifacts\"\"\"\n",
    "        try:\n",
    "            # Load model\n",
    "            self.model = joblib.load(self.model_dir / 'bid_fee_model.joblib')\n",
    "            \n",
    "            # Load encoders\n",
    "            self.encoders = joblib.load(self.model_dir / 'encoders.joblib')\n",
    "            \n",
    "            # Load configuration\n",
    "            with open(config_dir / 'model_config.json', 'r') as f:\n",
    "                self.config = json.load(f)\n",
    "            \n",
    "            self.feature_cols = self.config['feature_cols']\n",
    "            self.categorical_cols = self.config['categorical_cols']\n",
    "            self.market_cols = self.config['market_cols']\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load model artifacts: {str(e)}\")\n",
    "    \n",
    "    def _prepare_features(self, data: dict) -> pd.DataFrame:\n",
    "        \"\"\"Prepare features for prediction\"\"\"\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame([data])\n",
    "        \n",
    "        # Process date features\n",
    "        if 'BidDate' in df.columns:\n",
    "            df['BidDate'] = pd.to_datetime(df['BidDate'])\n",
    "            df['Year'] = df['BidDate'].dt.year\n",
    "            df['Month'] = df['BidDate'].dt.month\n",
    "            df['Week'] = df['BidDate'].dt.isocalendar().week\n",
    "            df['DayOfWeek'] = df['BidDate'].dt.dayofweek\n",
    "        \n",
    "        # Process categorical columns\n",
    "        for col in self.categorical_cols:\n",
    "            if col in df.columns and col in self.encoders:\n",
    "                df[f'{col}_encoded'] = df[col].astype(str).map(\n",
    "                    lambda x: self.encoders[col].transform([x])[0] \n",
    "                    if x in self.encoders[col].classes_ else -1\n",
    "                )\n",
    "        \n",
    "        # Calculate market-based features\n",
    "        for col in self.market_cols:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_zip_ratio'] = 1.0  # Default for single predictions\n",
    "        \n",
    "        # Ensure all feature columns exist\n",
    "        for col in self.feature_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0\n",
    "        \n",
    "        return df[self.feature_cols]\n",
    "    \n",
    "    def predict(self, data: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Make a bid fee prediction\n",
    "        \n",
    "        Args:\n",
    "            data: Dictionary containing bid opportunity data\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with prediction and confidence info\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate required fields\n",
    "            required_fields = {'ZipCode', 'PropertyType', 'BidDate'}\n",
    "            missing = required_fields - set(data.keys())\n",
    "            if missing:\n",
    "                raise ValueError(f\"Missing required fields: {missing}\")\n",
    "            \n",
    "            # Prepare features\n",
    "            X = self._prepare_features(data)\n",
    "            \n",
    "            # Make prediction\n",
    "            pred = float(self.model.predict(X)[0])\n",
    "            \n",
    "            # Get prediction probabilities\n",
    "            pred_proba = self.model.predict_proba(X)[0] if hasattr(self.model, 'predict_proba') else None\n",
    "            confidence = float(pred_proba.max()) if pred_proba is not None else 0.8\n",
    "            \n",
    "            # Get error bounds from config\n",
    "            error_percentiles = self.config['performance']['error_percentiles']\n",
    "            \n",
    "            return {\n",
    "                'predicted_fee': pred,\n",
    "                'confidence_score': confidence,\n",
    "                'prediction_interval': {\n",
    "                    'lower_bound': max(0, pred - error_percentiles['p90']),\n",
    "                    'upper_bound': pred + error_percentiles['p90']\n",
    "                },\n",
    "                'model_version': self.config['model_info']['version'],\n",
    "                'timestamp': datetime.datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Prediction failed: {str(e)}\")\n",
    "\n",
    "# 3. Test the production class\n",
    "print(\"\\nTesting prediction class...\")\n",
    "\n",
    "predictor = BidPrediction('deployment/models')\n",
    "\n",
    "# Test with sample data\n",
    "sample_data = {\n",
    "    'ZipCode': '12345',\n",
    "    'PropertyType': 'Office',\n",
    "    'BidDate': '2025-10-23',\n",
    "    'DistanceInMiles': 10.5,\n",
    "    'Market': 'NYC',\n",
    "    'PopulationEstimate': 50000,\n",
    "    'AverageHouseValue': 500000\n",
    "}\n",
    "\n",
    "try:\n",
    "    result = predictor.predict(sample_data)\n",
    "    print(\"\\nTest prediction successful!\")\n",
    "    print(f\"Predicted bid fee: ${result['predicted_fee']:.2f}\")\n",
    "    print(f\"Confidence score: {result['confidence_score']:.2%}\")\n",
    "    print(f\"90% Prediction interval: ${result['prediction_interval']['lower_bound']:.2f} to ${result['prediction_interval']['upper_bound']:.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Test prediction failed: {str(e)}\")\n",
    "\n",
    "# Save predictor to global scope\n",
    "globals().update({\n",
    "    'predictor': predictor\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfdc778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running comprehensive model validation...\n",
      "\n",
      "Validating data representation...\n",
      "\n",
      "Testing different property scenarios...\n",
      "\n",
      "Testing Basic Office Property:\n",
      "Prediction: $2580.26\n",
      "Confidence: 80.00%\n",
      "Range: $813.01 to $4347.52\n",
      "\n",
      "Testing Complex Property with Market Data:\n",
      "Prediction: $2381.43\n",
      "Confidence: 80.00%\n",
      "Range: $614.18 to $4148.69\n",
      "\n",
      "Testing Edge Case with Missing Fields:\n",
      "Prediction: $2813.36Prediction: $2813.36\n",
      "Confidence: 80.00%\n",
      "Range: $1046.11 to $4580.61\n",
      "\n",
      "Testing error handling...\n",
      "\n",
      "Testing Missing Required Field:\n",
      "Expected error caught: Prediction failed: Missing required fields: {'ZipCode'}\n",
      "\n",
      "Testing Invalid Date Format:\n",
      "Expected error caught: Prediction failed: Unknown datetime string format, unable to parse: invalid-date, at position 0\n",
      "\n",
      "Testing Unknown Property Type:\n",
      "Warning: Expected error but got result:\n",
      "{'predicted_fee': 3446.03662109375, 'confidence_score': 0.8, 'prediction_interval': {'lower_bound': 1678.7840820312501, 'upper_bound': 5213.28916015625}, 'model_version': '1.0.0', 'timestamp': '2025-10-23T13:51:56.845221'}\n",
      "\n",
      "Validating model artifacts...\n",
      "✓ deployment/models/bid_fee_model.joblib exists\n",
      "  Size: 904.0 KB\n",
      "  Modified: 2025-10-23T13:51:56.365976\n",
      "✓ deployment/models/encoders.joblib exists\n",
      "  Size: 159.7 KB\n",
      "  Modified: 2025-10-23T13:51:56.382513\n",
      "✓ deployment/config/model_config.json exists\n",
      "  Size: 3.3 KB\n",
      "  Modified: 2025-10-23T13:51:56.371968\n",
      "\n",
      "Model configuration loaded successfully:\n",
      "Version: 1.0.0\n",
      "Created: 2025-10-23T13:51:56.367970\n",
      "\n",
      "Model Performance:\n",
      "Average RMSE: $1396.88\n",
      "Average MAE: $875.66\n",
      "Average R²: 0.424\n",
      "\n",
      "Validation complete!\n",
      "\n",
      "Confidence: 80.00%\n",
      "Range: $1046.11 to $4580.61\n",
      "\n",
      "Testing error handling...\n",
      "\n",
      "Testing Missing Required Field:\n",
      "Expected error caught: Prediction failed: Missing required fields: {'ZipCode'}\n",
      "\n",
      "Testing Invalid Date Format:\n",
      "Expected error caught: Prediction failed: Unknown datetime string format, unable to parse: invalid-date, at position 0\n",
      "\n",
      "Testing Unknown Property Type:\n",
      "Warning: Expected error but got result:\n",
      "{'predicted_fee': 3446.03662109375, 'confidence_score': 0.8, 'prediction_interval': {'lower_bound': 1678.7840820312501, 'upper_bound': 5213.28916015625}, 'model_version': '1.0.0', 'timestamp': '2025-10-23T13:51:56.845221'}\n",
      "\n",
      "Validating model artifacts...\n",
      "✓ deployment/models/bid_fee_model.joblib exists\n",
      "  Size: 904.0 KB\n",
      "  Modified: 2025-10-23T13:51:56.365976\n",
      "✓ deployment/models/encoders.joblib exists\n",
      "  Size: 159.7 KB\n",
      "  Modified: 2025-10-23T13:51:56.382513\n",
      "✓ deployment/config/model_config.json exists\n",
      "  Size: 3.3 KB\n",
      "  Modified: 2025-10-23T13:51:56.371968\n",
      "\n",
      "Model configuration loaded successfully:\n",
      "Version: 1.0.0\n",
      "Created: 2025-10-23T13:51:56.367970\n",
      "\n",
      "Model Performance:\n",
      "Average RMSE: $1396.88\n",
      "Average MAE: $875.66\n",
      "Average R²: 0.424\n",
      "\n",
      "Validation complete!\n"
     ]
    }
   ],
   "source": [
    "# Model Validation and Testing\n",
    "\n",
    "print(\"Running comprehensive model validation...\")\n",
    "\n",
    "# 1. Test data representation\n",
    "print(\"\\nValidating data representation...\")\n",
    "test_cases = [\n",
    "    {\n",
    "        'case': 'Basic Office Property',\n",
    "        'data': {\n",
    "            'ZipCode': '12345',\n",
    "            'PropertyType': 'Office',\n",
    "            'BidDate': '2025-10-23',\n",
    "            'DistanceInMiles': 10.5,\n",
    "            'Market': 'NYC'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'case': 'Complex Property with Market Data',\n",
    "        'data': {\n",
    "            'ZipCode': '90210',\n",
    "            'PropertyType': 'Retail',\n",
    "            'BidDate': '2025-10-23',\n",
    "            'DistanceInMiles': 5.2,\n",
    "            'Market': 'LA',\n",
    "            'PopulationEstimate': 75000,\n",
    "            'AverageHouseValue': 1500000,\n",
    "            'IncomePerHousehold': 120000\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'case': 'Edge Case with Missing Fields',\n",
    "        'data': {\n",
    "            'ZipCode': '54321',\n",
    "            'PropertyType': 'Industrial',\n",
    "            'BidDate': '2025-10-23',\n",
    "            'DistanceInMiles': 15.7\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\nTesting different property scenarios...\")\n",
    "for test in test_cases:\n",
    "    print(f\"\\nTesting {test['case']}:\")\n",
    "    try:\n",
    "        result = predictor.predict(test['data'])\n",
    "        print(f\"Prediction: ${result['predicted_fee']:.2f}\")\n",
    "        print(f\"Confidence: {result['confidence_score']:.2%}\")\n",
    "        print(f\"Range: ${result['prediction_interval']['lower_bound']:.2f} to ${result['prediction_interval']['upper_bound']:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "# 2. Test error handling\n",
    "print(\"\\nTesting error handling...\")\n",
    "error_cases = [\n",
    "    {\n",
    "        'case': 'Missing Required Field',\n",
    "        'data': {'PropertyType': 'Office', 'BidDate': '2025-10-23'}\n",
    "    },\n",
    "    {\n",
    "        'case': 'Invalid Date Format',\n",
    "        'data': {\n",
    "            'ZipCode': '12345',\n",
    "            'PropertyType': 'Office',\n",
    "            'BidDate': 'invalid-date'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'case': 'Unknown Property Type',\n",
    "        'data': {\n",
    "            'ZipCode': '12345',\n",
    "            'PropertyType': 'Unknown',\n",
    "            'BidDate': '2025-10-23'\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "for test in error_cases:\n",
    "    print(f\"\\nTesting {test['case']}:\")\n",
    "    try:\n",
    "        result = predictor.predict(test['data'])\n",
    "        print(\"Warning: Expected error but got result:\")\n",
    "        print(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Expected error caught: {str(e)}\")\n",
    "\n",
    "# 3. Validate model artifacts\n",
    "print(\"\\nValidating model artifacts...\")\n",
    "required_files = [\n",
    "    'deployment/models/bid_fee_model.joblib',\n",
    "    'deployment/models/encoders.joblib',\n",
    "    'deployment/config/model_config.json'\n",
    "]\n",
    "\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"✓ {file} exists\")\n",
    "        print(f\"  Size: {os.path.getsize(file) / 1024:.1f} KB\")\n",
    "        print(f\"  Modified: {datetime.datetime.fromtimestamp(os.path.getmtime(file)).isoformat()}\")\n",
    "    else:\n",
    "        print(f\"✗ Missing required file: {file}\")\n",
    "\n",
    "# 4. Load and validate configuration\n",
    "try:\n",
    "    with open('deployment/config/model_config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    print(\"\\nModel configuration loaded successfully:\")\n",
    "    print(f\"Version: {config['model_info']['version']}\")\n",
    "    print(f\"Created: {config['model_info']['created_at']}\")\n",
    "    print(\"\\nModel Performance:\")\n",
    "    print(f\"Average RMSE: ${config['performance']['cv_scores']['rmse_mean']:.2f}\")\n",
    "    print(f\"Average MAE: ${config['performance']['cv_scores']['mae_mean']:.2f}\")\n",
    "    print(f\"Average R²: {config['performance']['cv_scores']['r2_mean']:.3f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error validating configuration: {str(e)}\")\n",
    "\n",
    "print(\"\\nValidation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5556bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for finding optimal fee\n",
    "def find_optimal_fee(opportunity_row, base_multiplier=2.0, steps=100):\n",
    "    \"\"\"\n",
    "    Find optimal bid fee for an opportunity\n",
    "    \n",
    "    Args:\n",
    "        opportunity_row: pandas Series with features\n",
    "        base_multiplier: How much above/below base fee to search\n",
    "        steps: Number of fee values to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        dict with optimal fee and expected value\n",
    "    \"\"\"\n",
    "    # Get base fee for scaling\n",
    "    base_fee = opportunity_row['BaseFee']\n",
    "    if np.isnan(base_fee) or base_fee <= 0:\n",
    "        base_fee = df['BaseFee'].median()  # Fallback to median\n",
    "        \n",
    "    # Generate fee range to evaluate\n",
    "    min_fee = max(1.0, base_fee / base_multiplier)  # Minimum $1 fee\n",
    "    max_fee = base_fee * base_multiplier\n",
    "    fee_range = np.linspace(min_fee, max_fee, steps)\n",
    "    \n",
    "    # Initialize arrays for results\n",
    "    win_probs = np.zeros_like(fee_range)\n",
    "    evs = np.zeros_like(fee_range)\n",
    "    \n",
    "    # Create feature matrix for each fee\n",
    "    X_eval = pd.concat([\n",
    "        pd.DataFrame([opportunity_row.to_dict()] * len(fee_range))\n",
    "        .assign(BidFee=fee_range)\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Get win probabilities if classifier exists\n",
    "    if clf is not None:\n",
    "        win_probs = clf.predict_proba(X_eval)[:,1]\n",
    "    else:\n",
    "        win_probs = np.full_like(fee_range, fallback_winprob)\n",
    "        \n",
    "    # Calculate expected values\n",
    "    evs = win_probs * fee_range\n",
    "    \n",
    "    # Find optimal point\n",
    "    best_idx = np.argmax(evs)\n",
    "    optimal_fee = fee_range[best_idx]\n",
    "    optimal_ev = evs[best_idx]\n",
    "    optimal_prob = win_probs[best_idx]\n",
    "    \n",
    "    return {\n",
    "        'best_fee': optimal_fee,\n",
    "        'best_ev': optimal_ev,\n",
    "        'win_prob': optimal_prob,\n",
    "        'fee_grid': fee_range,\n",
    "        'win_probs': win_probs,\n",
    "        'evs': evs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869222f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count: 6\n",
      "No week_win_rate available for classifier\n"
     ]
    }
   ],
   "source": [
    "# Build feature matrix X and targets\n",
    "drop_cols = ['ZipCode','BidWeekStart']\n",
    "target_reg = 'median_BidFee'\n",
    "features = [c for c in agg.columns if c not in drop_cols + [target_reg]]\n",
    "features = sorted(features)\n",
    "print('Feature count:', len(features))\n",
    "\n",
    "# Prepare matrix X and target y\n",
    "X = agg[features].copy()\n",
    "y_reg = agg[target_reg].copy()\n",
    "\n",
    "# Build classifier labels if week_win_rate exists\n",
    "if 'week_win_rate' in agg.columns and agg['week_win_rate'].notna().sum() > 0:\n",
    "    threshold = agg['week_win_rate'].median()\n",
    "    y_clf = (agg['week_win_rate'] > threshold).astype(int)\n",
    "    print('Using week_win_rate median threshold:', threshold)\n",
    "else:\n",
    "    y_clf = None\n",
    "    print('No week_win_rate available for classifier')\n",
    "\n",
    "# Frequency-encode any remaining object columns\n",
    "encoders = {}\n",
    "for c in X.columns:\n",
    "    if X[c].dtype == 'object' or X[c].dtype.name == 'category':\n",
    "        vc = X[c].astype(str).fillna('MISSING').value_counts(normalize=True)\n",
    "        encoders[c] = vc.to_dict()\n",
    "        X[c] = X[c].astype(str).fillna('MISSING').map(encoders[c]).fillna(0.0)\n",
    "\n",
    "# Fill numeric nans with medians\n",
    "train_medians = X.median()\n",
    "X = X.fillna(train_medians)\n",
    "\n",
    "# Persist pieces for downstream cells\n",
    "globals().update({\n",
    "    'features': features,\n",
    "    'encoders': encoders,\n",
    "    'train_medians': train_medians,\n",
    "    'agg': agg,\n",
    "    'X': X,\n",
    "    'y_reg': y_reg,\n",
    "    'y_clf': y_clf\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4db003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved regressor model\n",
      "\n",
      "Saved model artifacts to models/bid_recommendation_artifacts.joblib\n",
      "\n",
      "Example usage & validation:\n",
      "\n",
      "Testing with ZIP code: 64503\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Missing features: {'roll_4w', 'lag_2', 'lag_1', 'week_win_rate', 'lag_3', 'lag_4'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[141], line 97\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# 2. Get predictions\u001b[39;00m\n\u001b[0;32m     96\u001b[0m predictor \u001b[38;5;241m=\u001b[39m BidPrediction()\n\u001b[1;32m---> 97\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_opportunity\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPredictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted bid: $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_bid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[141], line 67\u001b[0m, in \u001b[0;36mBidPrediction.predict\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mMake prediction for new data\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    dict with predictions\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Validate features\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m X_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Clean features\u001b[39;00m\n\u001b[0;32m     70\u001b[0m X_new \u001b[38;5;241m=\u001b[39m X_new\u001b[38;5;241m.\u001b[39mreplace([np\u001b[38;5;241m.\u001b[39minf, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf], np\u001b[38;5;241m.\u001b[39mnan)\n",
      "Cell \u001b[1;32mIn[141], line 51\u001b[0m, in \u001b[0;36mBidPrediction.validate_features\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     49\u001b[0m missing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(data\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m extra \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(data\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra:\n",
      "\u001b[1;31mValueError\u001b[0m: Missing features: {'roll_4w', 'lag_2', 'lag_1', 'week_win_rate', 'lag_3', 'lag_4'}"
     ]
    }
   ],
   "source": [
    "# Production Setup: Save models and create inference wrapper\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save regressor model\n",
    "joblib.dump(model_full, 'models/regressor.joblib')\n",
    "print(\"Saved regressor model\")\n",
    "\n",
    "# Save classifier if available\n",
    "if 'clf' in globals() and clf is not None:\n",
    "    joblib.dump(clf, 'models/classifier.joblib')\n",
    "    print(\"Saved classifier model\")\n",
    "    fallback_winprob = 0.5  # default win probability when classifier unavailable\n",
    "else:\n",
    "    fallback_winprob = y_clf.mean() if y_clf is not None else 0.5\n",
    "\n",
    "# Save artifacts for inference\n",
    "artifacts = {\n",
    "    'feature_names': list(X.columns),\n",
    "    'target_reg': target_reg,\n",
    "    'has_classifier': 'clf' in globals() and clf is not None,\n",
    "    'fallback_winprob': fallback_winprob\n",
    "}\n",
    "joblib.dump(artifacts, 'models/bid_recommendation_artifacts.joblib')\n",
    "print(\"\\nSaved model artifacts to models/bid_recommendation_artifacts.joblib\")\n",
    "\n",
    "# Create inference wrapper class\n",
    "class BidPrediction:\n",
    "    def __init__(self):\n",
    "        self.regressor = joblib.load('models/regressor.joblib')\n",
    "        try:\n",
    "            self.classifier = joblib.load('models/classifier.joblib')\n",
    "            self.has_classifier = True\n",
    "        except:\n",
    "            self.classifier = None\n",
    "            self.has_classifier = False\n",
    "        \n",
    "        # Load artifacts\n",
    "        self.artifacts = joblib.load('models/bid_recommendation_artifacts.joblib')\n",
    "        self.feature_names = self.artifacts['feature_names']\n",
    "        self.target_reg = self.artifacts['target_reg']\n",
    "        self.fallback_winprob = self.artifacts['fallback_winprob']\n",
    "    \n",
    "    def validate_features(self, data):\n",
    "        \"\"\"Validate input features\"\"\"\n",
    "        missing = set(self.feature_names) - set(data.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing features: {missing}\")\n",
    "        extra = set(data.columns) - set(self.feature_names)\n",
    "        if extra:\n",
    "            raise ValueError(f\"Unexpected features: {extra}\")\n",
    "            \n",
    "        return data[self.feature_names]\n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Make prediction for new data\n",
    "        Args:\n",
    "            data: pandas DataFrame with required features\n",
    "        Returns:\n",
    "            dict with predictions\n",
    "        \"\"\"\n",
    "        # Validate features\n",
    "        X_new = self.validate_features(data)\n",
    "        \n",
    "        # Clean features\n",
    "        X_new = X_new.replace([np.inf, -np.inf], np.nan)\n",
    "        X_new = X_new.fillna(X_new.median())\n",
    "        \n",
    "        # Make predictions\n",
    "        bid_pred = float(self.regressor.predict(X_new)[0])\n",
    "        result = {'predicted_bid': bid_pred}\n",
    "        \n",
    "        # Add win probability if classifier exists\n",
    "        if self.has_classifier:\n",
    "            win_prob = float(self.classifier.predict_proba(X_new)[:,1][0])\n",
    "            result['win_probability'] = win_prob\n",
    "        else:\n",
    "            result['win_probability'] = self.fallback_winprob\n",
    "            \n",
    "        return result\n",
    "\n",
    "# Print example usage & validation\n",
    "print(\"\\nExample usage & validation:\\n\")\n",
    "\n",
    "# 1. Create test data\n",
    "zip_sample = df['ZipCode'].sample(n=1).iloc[0]  # random ZIP\n",
    "print(f\"Testing with ZIP code: {zip_sample}\")\n",
    "\n",
    "sample_opportunity = df[df['ZipCode']==zip_sample].sort_values('BidWeekStart').iloc[-1]\n",
    "\n",
    "# 2. Get predictions\n",
    "predictor = BidPrediction()\n",
    "prediction = predictor.predict(pd.DataFrame([sample_opportunity]))\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "print(f\"Predicted bid: ${prediction['predicted_bid']:.2f}\")\n",
    "print(f\"Win probability: {prediction['win_probability']:.1%}\")\n",
    "\n",
    "# 3. Get bid recommendation with EV optimization\n",
    "recommendations = find_optimal_fee(sample_opportunity)\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "print(f\"Optimal bid fee: ${recommendations['best_fee']:.2f}\")\n",
    "print(f\"Expected value: ${recommendations['best_ev']:.2f}\")\n",
    "print(f\"Win probability at optimal fee: {recommendations['win_prob']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72bc550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Regressor...\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[12:33:39] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\data\\data.cc:550: Check failed: valid: Label contains NaN, infinity or a value too large.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 18\u001b[0m\n\u001b[0;32m      9\u001b[0m ytr, yte \u001b[38;5;241m=\u001b[39m y_reg\u001b[38;5;241m.\u001b[39miloc[tr], y_reg\u001b[38;5;241m.\u001b[39miloc[te]\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBRegressor(\n\u001b[0;32m     12\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, \n\u001b[0;32m     13\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     17\u001b[0m )\n\u001b[1;32m---> 18\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(Xte)\n\u001b[0;32m     21\u001b[0m rmse \u001b[38;5;241m=\u001b[39m mean_squared_error(yte, preds, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\sklearn.py:1222\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1217\u001b[0m model, metric, params, feature_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(\n\u001b[0;32m   1218\u001b[0m     xgb_model, params, feature_weights\n\u001b[0;32m   1219\u001b[0m )\n\u001b[0;32m   1221\u001b[0m evals_result: TrainingCallback\u001b[38;5;241m.\u001b[39mEvalsLog \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1222\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_evaluation_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1231\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1232\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_qid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_dmatrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1242\u001b[0m     obj: Optional[Objective] \u001b[38;5;241m=\u001b[39m _objective_decorator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\sklearn.py:628\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[1;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_evaluation_matrices\u001b[39m(\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    609\u001b[0m     missing: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[0;32m    625\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m    626\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;124;03m    way.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m     train_dmatrix \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dmatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    642\u001b[0m     n_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_set)\n\u001b[0;32m    644\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\sklearn.py:1137\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[1;34m(self, ref, **kwargs)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _can_use_qdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbooster \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1136\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1137\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQuantileDMatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_bin\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1140\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:1614\u001b[0m, in \u001b[0;36mQuantileDMatrix.__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, max_quantile_batches, data_split_mode)\u001b[0m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m   1595\u001b[0m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1596\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1607\u001b[0m         )\n\u001b[0;32m   1608\u001b[0m     ):\n\u001b[0;32m   1609\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1610\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf data iterator is used as input, data like label should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1611\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified as batch argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1612\u001b[0m         )\n\u001b[1;32m-> 1614\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_quantile_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_quantile_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:1678\u001b[0m, in \u001b[0;36mQuantileDMatrix._init\u001b[1;34m(self, data, ref, enable_categorical, max_quantile_blocks, **meta)\u001b[0m\n\u001b[0;32m   1663\u001b[0m config \u001b[38;5;241m=\u001b[39m make_jcargs(\n\u001b[0;32m   1664\u001b[0m     nthread\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnthread,\n\u001b[0;32m   1665\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1666\u001b[0m     max_bin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_bin,\n\u001b[0;32m   1667\u001b[0m     max_quantile_blocks\u001b[38;5;241m=\u001b[39mmax_quantile_blocks,\n\u001b[0;32m   1668\u001b[0m )\n\u001b[0;32m   1669\u001b[0m ret \u001b[38;5;241m=\u001b[39m _LIB\u001b[38;5;241m.\u001b[39mXGQuantileDMatrixCreateFromCallback(\n\u001b[0;32m   1670\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1671\u001b[0m     it\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1676\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mbyref(handle),\n\u001b[0;32m   1677\u001b[0m )\n\u001b[1;32m-> 1678\u001b[0m \u001b[43mit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1679\u001b[0m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[0;32m   1680\u001b[0m _check_call(ret)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:572\u001b[0m, in \u001b[0;36mDataIter.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    570\u001b[0m exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:553\u001b[0m, in \u001b[0;36mDataIter._handle_exception\u001b[1;34m(self, fn, dft_ret)\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dft_ret\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;66;03m# Defer the exception in order to return 0 and stop the iteration.\u001b[39;00m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;66;03m# Exception inside a ctype callback function has no effect except\u001b[39;00m\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;66;03m# for printing to stderr (doesn't stop the execution).\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     tb \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:640\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m--> 640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_exception(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\data.py:1654\u001b[0m, in \u001b[0;36mSingleBatchInternalIter.next\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m   1652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mit \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1654\u001b[0m \u001b[43minput_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:629\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.input_data\u001b[1;34m(data, feature_names, feature_types, **kwargs)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data \u001b[38;5;241m=\u001b[39m (new, cat_codes, feature_names, feature_types)\n\u001b[0;32m    628\u001b[0m dispatch_proxy_set_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy, new, cat_codes)\n\u001b[1;32m--> 629\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_ref \u001b[38;5;241m=\u001b[39m ref\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:961\u001b[0m, in \u001b[0;36mDMatrix.set_info\u001b[1;34m(self, label, weight, base_margin, group, qid, label_lower_bound, label_upper_bound, feature_names, feature_types, feature_weights)\u001b[0m\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch_meta_backend\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 961\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_weight(weight)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:1099\u001b[0m, in \u001b[0;36mDMatrix.set_label\u001b[1;34m(self, label)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Set label of dmatrix\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;124;03m    The label information to be set into DMatrix\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch_meta_backend\n\u001b[1;32m-> 1099\u001b[0m \u001b[43mdispatch_meta_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\data.py:1603\u001b[0m, in \u001b[0;36mdispatch_meta_backend\u001b[1;34m(matrix, data, name, dtype)\u001b[0m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_series(data):\n\u001b[1;32m-> 1603\u001b[0m     \u001b[43m_meta_from_pandas_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_dlpack(data):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\data.py:713\u001b[0m, in \u001b[0;36m_meta_from_pandas_series\u001b[1;34m(data, name, dtype, handle)\u001b[0m\n\u001b[0;32m    711\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto_dense()  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 713\u001b[0m \u001b[43m_meta_from_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\data.py:1533\u001b[0m, in \u001b[0;36m_meta_from_numpy\u001b[1;34m(data, field, dtype, handle)\u001b[0m\n\u001b[0;32m   1531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMasked array is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1532\u001b[0m interface_str \u001b[38;5;241m=\u001b[39m array_interface(data)\n\u001b[1;32m-> 1533\u001b[0m \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGDMatrixSetInfoFromInterface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:310\u001b[0m, in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m    300\u001b[0m \n\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [12:33:39] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\data\\data.cc:550: Check failed: valid: Label contains NaN, infinity or a value too large."
     ]
    }
   ],
   "source": [
    "# Model Training with Error Handling\n",
    "\n",
    "def safe_train_model(X, y, model_type='regressor'):\n",
    "    \"\"\"Train a model with proper error handling and validation\"\"\"\n",
    "    \n",
    "    # 1. Validate inputs\n",
    "    if X is None or y is None:\n",
    "        raise ValueError(\"X and y must not be None\")\n",
    "    \n",
    "    if len(X) != len(y):\n",
    "        raise ValueError(f\"X and y must have same length. Got X: {len(X)}, y: {len(y)}\")\n",
    "    \n",
    "    # 2. Clean target variable\n",
    "    y = pd.Series(y)\n",
    "    y = y.replace([np.inf, -np.inf], np.nan)\n",
    "    y = y.fillna(y.median())\n",
    "    \n",
    "    # 3. Clean features\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    X = X.fillna(X.median())\n",
    "    \n",
    "    # 4. Create model\n",
    "    if model_type == 'regressor':\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.08,\n",
    "            max_depth=4,\n",
    "            scale_pos_weight=1.0,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    # 5. Fit model with error handling\n",
    "    try:\n",
    "        model.fit(X, y)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {model_type}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Train regressor with time-series CV\n",
    "print(\"\\nTraining Regressor...\")\n",
    "\n",
    "try:\n",
    "    # Clean target variable\n",
    "    y_reg = y_reg.replace([np.inf, -np.inf], np.nan)\n",
    "    y_reg = y_reg.fillna(y_reg.median())\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    rmse_list = []\n",
    "    mae_list = []\n",
    "    \n",
    "    for fold, (tr, te) in enumerate(tscv.split(X), 1):\n",
    "        try:\n",
    "            Xtr, Xte = X.iloc[tr], X.iloc[te]\n",
    "            ytr, yte = y_reg.iloc[tr], y_reg.iloc[te]\n",
    "            \n",
    "            model = safe_train_model(Xtr, ytr, 'regressor')\n",
    "            if model is None:\n",
    "                continue\n",
    "                \n",
    "            preds = model.predict(Xte)\n",
    "            rmse = mean_squared_error(yte, preds, squared=False)\n",
    "            mae = mean_absolute_error(yte, preds)\n",
    "            rmse_list.append(rmse)\n",
    "            mae_list.append(mae)\n",
    "            print(f'Fold {fold}: RMSE={rmse:.3f}, MAE={mae:.3f}')\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fold {fold}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if rmse_list:\n",
    "        print(f'\\nAverage RMSE: {np.mean(rmse_list):.3f}')\n",
    "        print(f'Average MAE: {np.mean(mae_list):.3f}')\n",
    "    else:\n",
    "        print(\"No successful folds\")\n",
    "        \n",
    "    # Train final regressor on all data\n",
    "    print(\"\\nTraining final model...\")\n",
    "    model_full = safe_train_model(X, y_reg, 'regressor')\n",
    "    if model_full is not None:\n",
    "        globals()['model_full'] = model_full\n",
    "        print(\"Final regressor trained successfully\")\n",
    "    else:\n",
    "        print(\"Failed to train final regressor\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in regressor training: {str(e)}\")\n",
    "    \n",
    "# Train classifier if we have labels\n",
    "if y_clf is not None and not y_clf.isna().all():\n",
    "    print(\"\\nTraining Classifier...\")\n",
    "    \n",
    "    try:\n",
    "        X_clf = X.copy()\n",
    "        y_clf = y_clf.fillna(0)  # Assume no win for missing values\n",
    "        \n",
    "        try:\n",
    "            Xtr, Xte, ytr, yte = train_test_split(\n",
    "                X_clf, y_clf,\n",
    "                test_size=0.2,\n",
    "                random_state=42,\n",
    "                stratify=y_clf\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Stratified split failed, using random split:\", e)\n",
    "            Xtr, Xte, ytr, yte = train_test_split(\n",
    "                X_clf, y_clf,\n",
    "                test_size=0.2,\n",
    "                random_state=42\n",
    "            )\n",
    "        \n",
    "        clf = safe_train_model(Xtr, ytr, 'classifier')\n",
    "        if clf is not None:\n",
    "            yproba = clf.predict_proba(Xte)[:,1]\n",
    "            try:\n",
    "                auc = roc_auc_score(yte, yproba)\n",
    "                print(f'ROC-AUC Score: {auc:.3f}')\n",
    "            except Exception as e:\n",
    "                print(\"Could not compute AUC:\", e)\n",
    "            \n",
    "            globals()['clf'] = clf\n",
    "            print(\"Classifier trained successfully\")\n",
    "        else:\n",
    "            print(\"Failed to train classifier\")\n",
    "            globals()['clf'] = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in classifier training: {str(e)}\")\n",
    "        globals()['clf'] = None\n",
    "else:\n",
    "    print(\"\\nNo labels for classifier training\")\n",
    "    globals()['clf'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b315c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production Setup: Save models and create inference wrapper\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    # Validate models exist\n",
    "    if 'model_full' not in globals() or model_full is None:\n",
    "        raise ValueError(\"Regressor model not available\")\n",
    "        \n",
    "    # Save regressor model\n",
    "    joblib.dump(model_full, 'models/regressor.joblib')\n",
    "    print(\"Saved regressor model\")\n",
    "    \n",
    "    # Save classifier if available\n",
    "    if 'clf' in globals() and clf is not None:\n",
    "        joblib.dump(clf, 'models/classifier.joblib')\n",
    "        print(\"Saved classifier model\")\n",
    "        fallback_winprob = 0.5\n",
    "    else:\n",
    "        fallback_winprob = y_clf.mean() if y_clf is not None else 0.5\n",
    "    \n",
    "    # Save artifacts for inference\n",
    "    artifacts = {\n",
    "        'feature_names': list(X.columns),\n",
    "        'target_reg': target_reg,\n",
    "        'has_classifier': 'clf' in globals() and clf is not None,\n",
    "        'fallback_winprob': fallback_winprob,\n",
    "        'categorical_columns': cat_cols,\n",
    "        'encoders': encoders,\n",
    "        'feature_statistics': {\n",
    "            col: {'median': X[col].median()} \n",
    "            for col in X.columns\n",
    "        }\n",
    "    }\n",
    "    joblib.dump(artifacts, 'models/bid_recommendation_artifacts.joblib')\n",
    "    print(\"\\nSaved model artifacts\")\n",
    "    \n",
    "    class BidPrediction:\n",
    "        \"\"\"Production inference class with robust error handling\"\"\"\n",
    "        \n",
    "        def __init__(self, models_dir='models'):\n",
    "            \"\"\"Load models and artifacts\"\"\"\n",
    "            try:\n",
    "                # Load regressor\n",
    "                self.regressor = joblib.load(f'{models_dir}/regressor.joblib')\n",
    "                \n",
    "                # Try to load classifier\n",
    "                try:\n",
    "                    self.classifier = joblib.load(f'{models_dir}/classifier.joblib')\n",
    "                    self.has_classifier = True\n",
    "                except:\n",
    "                    self.classifier = None\n",
    "                    self.has_classifier = False\n",
    "                \n",
    "                # Load artifacts\n",
    "                self.artifacts = joblib.load(f'{models_dir}/bid_recommendation_artifacts.joblib')\n",
    "                self.feature_names = self.artifacts['feature_names']\n",
    "                self.target_reg = self.artifacts['target_reg']\n",
    "                self.fallback_winprob = self.artifacts['fallback_winprob']\n",
    "                self.encoders = self.artifacts['encoders']\n",
    "                self.feature_stats = self.artifacts['feature_statistics']\n",
    "                \n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Error initializing BidPrediction: {str(e)}\")\n",
    "        \n",
    "        def preprocess_features(self, data):\n",
    "            \"\"\"Preprocess features with error handling\"\"\"\n",
    "            try:\n",
    "                # Validate required columns\n",
    "                missing_cols = set(self.artifacts['categorical_columns']) - set(data.columns)\n",
    "                if missing_cols:\n",
    "                    raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "                \n",
    "                # Create feature matrix\n",
    "                X_new = data.copy()\n",
    "                \n",
    "                # Encode categorical columns\n",
    "                for col, encoder in self.encoders.items():\n",
    "                    if col in data.columns:\n",
    "                        X_new[f'{col}_encoded'] = encoder.transform(data[col].astype(str))\n",
    "                \n",
    "                # Select features in correct order\n",
    "                X_new = X_new[self.feature_names]\n",
    "                \n",
    "                # Handle missing values using saved statistics\n",
    "                for col in X_new.columns:\n",
    "                    if X_new[col].isna().any():\n",
    "                        X_new[col] = X_new[col].fillna(self.feature_stats[col]['median'])\n",
    "                \n",
    "                return X_new\n",
    "                \n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Error preprocessing features: {str(e)}\")\n",
    "        \n",
    "        def predict(self, data):\n",
    "            \"\"\"\n",
    "            Make prediction for new data with comprehensive error handling\n",
    "            \n",
    "            Args:\n",
    "                data: pandas DataFrame with required features\n",
    "                \n",
    "            Returns:\n",
    "                dict with predictions and metadata\n",
    "            \"\"\"\n",
    "            try:\n",
    "                # Validate input\n",
    "                if not isinstance(data, pd.DataFrame):\n",
    "                    raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "                \n",
    "                # Preprocess features\n",
    "                X_new = self.preprocess_features(data)\n",
    "                \n",
    "                # Make predictions\n",
    "                bid_pred = float(self.regressor.predict(X_new)[0])\n",
    "                result = {\n",
    "                    'predicted_bid': bid_pred,\n",
    "                    'features_used': list(X_new.columns)\n",
    "                }\n",
    "                \n",
    "                # Add win probability if classifier exists\n",
    "                if self.has_classifier:\n",
    "                    win_prob = float(self.classifier.predict_proba(X_new)[:,1][0])\n",
    "                    result['win_probability'] = win_prob\n",
    "                else:\n",
    "                    result['win_probability'] = self.fallback_winprob\n",
    "                    result['win_probability_note'] = 'Using fallback probability'\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Error making prediction: {str(e)}\")\n",
    "    \n",
    "    # Test inference wrapper\n",
    "    print(\"\\nTesting inference wrapper...\")\n",
    "    predictor = BidPrediction()\n",
    "    \n",
    "    # Create test data\n",
    "    test_data = df.iloc[[0]]\n",
    "    print(\"\\nTest data shape:\", test_data.shape)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = predictor.predict(test_data)\n",
    "    print(\"\\nTest prediction successful:\")\n",
    "    for k, v in prediction.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in production setup: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d227fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Interpretability with SHAP values\n",
    "import shap\n",
    "\n",
    "print(\"Calculating SHAP values for regressor...\")\n",
    "explainer = shap.TreeExplainer(model_full)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12,6))\n",
    "shap.summary_plot(shap_values, X, show=False)\n",
    "plt.title('Feature Importance (SHAP)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot detailed SHAP values for first prediction\n",
    "plt.figure(figsize=(12,6))\n",
    "shap.force_plot(\n",
    "    explainer.expected_value, \n",
    "    shap_values[0,:],\n",
    "    X.iloc[0,:],\n",
    "    matplotlib=True,\n",
    "    show=False\n",
    ")\n",
    "plt.title('SHAP Force Plot for First Prediction')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if 'clf' in globals():\n",
    "    print(\"\\nCalculating SHAP values for classifier...\")\n",
    "    explainer_clf = shap.TreeExplainer(clf)\n",
    "    shap_values_clf = explainer_clf.shap_values(X)\n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "    shap.summary_plot(shap_values_clf[1], X, show=False)\n",
    "    plt.title('Feature Importance for Win Probability (SHAP)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be63cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Validation\n",
    "\n",
    "def validate_production_setup():\n",
    "    \"\"\"Comprehensive validation of the production setup\"\"\"\n",
    "    try:\n",
    "        print(\"Running final validation...\\n\")\n",
    "        checks = []\n",
    "        \n",
    "        # 1. Check models directory exists\n",
    "        models_dir_exists = os.path.exists('models')\n",
    "        checks.append(('Models directory exists', models_dir_exists))\n",
    "        \n",
    "        # 2. Check model files exist\n",
    "        model_files = {\n",
    "            'regressor.joblib': False,\n",
    "            'classifier.joblib': False,\n",
    "            'bid_recommendation_artifacts.joblib': False\n",
    "        }\n",
    "        for file in model_files:\n",
    "            model_files[file] = os.path.exists(os.path.join('models', file))\n",
    "            checks.append((f'{file} exists', model_files[file]))\n",
    "        \n",
    "        # 3. Test model loading\n",
    "        try:\n",
    "            predictor = BidPrediction()\n",
    "            checks.append(('Model loading', True))\n",
    "        except Exception as e:\n",
    "            checks.append(('Model loading', False))\n",
    "            print(f\"Model loading error: {str(e)}\")\n",
    "        \n",
    "        # 4. Test inference\n",
    "        if df is not None and len(df) > 0:\n",
    "            try:\n",
    "                test_data = df.iloc[[0]]\n",
    "                prediction = predictor.predict(test_data)\n",
    "                checks.append(('Inference test', True))\n",
    "                checks.append(('Prediction contains bid', 'predicted_bid' in prediction))\n",
    "                checks.append(('Prediction contains probability', 'win_probability' in prediction))\n",
    "            except Exception as e:\n",
    "                checks.append(('Inference test', False))\n",
    "                print(f\"Inference error: {str(e)}\")\n",
    "        \n",
    "        # Print results\n",
    "        print(\"Validation Results:\")\n",
    "        for check, status in checks:\n",
    "            print(f\"{check}: {'✓' if status else '✗'}\")\n",
    "        \n",
    "        # Overall status\n",
    "        success = all(status for _, status in checks)\n",
    "        print(f\"\\nOverall validation: {'Passed' if success else 'Failed'}\")\n",
    "        \n",
    "        return success\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Validation error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Run validation\n",
    "validation_success = validate_production_setup()\n",
    "\n",
    "if validation_success:\n",
    "    print(\"\\nModel is production-ready! ✨\")\n",
    "    print(\"\\nUsage example:\")\n",
    "    print(\"```python\")\n",
    "    print(\"from bid_prediction import BidPrediction\")\n",
    "    print(\"predictor = BidPrediction()\")\n",
    "    print(\"prediction = predictor.predict(new_data)\")\n",
    "    print(\"```\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Model needs attention before production use\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
